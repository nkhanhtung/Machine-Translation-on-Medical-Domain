{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12688285,
          "sourceType": "datasetVersion",
          "datasetId": 8018252
        },
        {
          "sourceId": 12732100,
          "sourceType": "datasetVersion",
          "datasetId": 8030370
        },
        {
          "sourceId": 12740598,
          "sourceType": "datasetVersion",
          "datasetId": 8053578
        },
        {
          "sourceId": 12740602,
          "sourceType": "datasetVersion",
          "datasetId": 8053581
        },
        {
          "sourceId": 12756926,
          "sourceType": "datasetVersion",
          "datasetId": 8053633
        },
        {
          "sourceId": 12760834,
          "sourceType": "datasetVersion",
          "datasetId": 8047748
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import các thư viện cần thiết"
      ],
      "metadata": {
        "id": "4mDjbiihuGsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:15.781887Z",
          "iopub.execute_input": "2025-08-14T06:26:15.782123Z",
          "iopub.status.idle": "2025-08-14T06:26:22.581558Z",
          "shell.execute_reply.started": "2025-08-14T06:26:15.782097Z",
          "shell.execute_reply": "2025-08-14T06:26:22.580985Z"
        },
        "id": "uzluahVukiH6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nltk"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:22.582782Z",
          "iopub.execute_input": "2025-08-14T06:26:22.583113Z",
          "iopub.status.idle": "2025-08-14T06:26:22.586440Z",
          "shell.execute_reply.started": "2025-08-14T06:26:22.583096Z",
          "shell.execute_reply": "2025-08-14T06:26:22.585867Z"
        },
        "id": "ak9jkLmgkiH8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Đọc dữ liệu từ tệp txt"
      ],
      "metadata": {
        "id": "vPMtPKcjuKAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_train_file = '/kaggle/input/data-nlp-midterm/train.en.txt'\n",
        "vi_train_file = '/kaggle/input/data-nlp-midterm/train.vi.txt'\n",
        "\n",
        "# Read lines\n",
        "with open(en_train_file, 'r', encoding='utf-8') as f_train_en:\n",
        "    en_train_lines = [line.strip() for line in f_train_en.readlines()]\n",
        "\n",
        "with open(vi_train_file, 'r', encoding='utf-8') as f_train_vi:\n",
        "    vi_train_lines = [line.strip() for line in f_train_vi.readlines()]\n",
        "\n",
        "# Check whether length of en_train equal to length of vi_train\n",
        "assert len(en_train_lines) == len(vi_train_lines)\n",
        "\n",
        "train = pd.DataFrame({'en': en_train_lines, 'vi': vi_train_lines})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:22.587191Z",
          "iopub.execute_input": "2025-08-14T06:26:22.587443Z",
          "iopub.status.idle": "2025-08-14T06:26:26.732314Z",
          "shell.execute_reply.started": "2025-08-14T06:26:22.587420Z",
          "shell.execute_reply": "2025-08-14T06:26:26.731759Z"
        },
        "id": "aWGQ_9M_kiH9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "en_test_file = '/kaggle/input/data-nlp-midterm/test.en.txt'\n",
        "vi_test_file = '/kaggle/input/data-nlp-midterm/test.vi.txt'\n",
        "\n",
        "# Read lines\n",
        "with open(en_test_file, 'r', encoding='utf-8') as f_test_en:\n",
        "    en_test_lines = [line.strip() for line in f_test_en.readlines()]\n",
        "\n",
        "with open(vi_test_file, 'r', encoding='utf-8') as f_test_vi:\n",
        "    vi_test_lines = [line.strip() for line in f_test_vi.readlines()]\n",
        "\n",
        "# Check whether length of en_train equal to length of vi_train\n",
        "assert len(en_test_lines) == len(vi_test_lines)\n",
        "\n",
        "test = pd.DataFrame({'en': en_test_lines, 'vi': vi_test_lines})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:26.733030Z",
          "iopub.execute_input": "2025-08-14T06:26:26.733236Z",
          "iopub.status.idle": "2025-08-14T06:26:26.793176Z",
          "shell.execute_reply.started": "2025-08-14T06:26:26.733218Z",
          "shell.execute_reply": "2025-08-14T06:26:26.792707Z"
        },
        "id": "PuasR-k9kiH9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:26.793812Z",
          "iopub.execute_input": "2025-08-14T06:26:26.794004Z",
          "iopub.status.idle": "2025-08-14T06:26:26.849692Z",
          "shell.execute_reply.started": "2025-08-14T06:26:26.793989Z",
          "shell.execute_reply": "2025-08-14T06:26:26.849007Z"
        },
        "id": "l3YDE2W3kiH9",
        "outputId": "c9f1c8cd-8c4d-4889-95ee-e461aa523737"
      },
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                       en  \\\n0       To evaluate clinical, subclinical symptoms of ...   \n1       Evaluate clinical, subclinical symptoms of pat...   \n2       There was a relation between vasodilatation an...   \n3       Otittis media effusion on V a is a common dise...   \n4       Main symptoms are rhinitis, nasal congestion, ...   \n...                                                   ...   \n499995  Patients and methods: Over-40-year-old men are...   \n499996  If chronic, the age at onset (eg, since birth,...   \n499997  Equipment for Removing a Tick Cleansing soluti...   \n499998                            Normal sigmoid at TVUS.   \n499999  Method: 19 patients underwent 25 coronectomy p...   \n\n                                                       vi  \n0       Nghiên cứu đặc điểm lâm sàng, cận lâm sàng bện...  \n1       Đánh giá đặc điểm lâm sàng, cận lâm sàng bệnh ...  \n2       Có sự liên quan giữa độ quá phát V.a với mức đ...  \n3       Kết luận: Viêm tai ứ dịch trên viêm V.a là bện...  \n4       Triệu chứng cơ năng nổi bật là chảy mũi, ngạt ...  \n...                                                   ...  \n499995  Đối tượng và phương pháp nghiên cứu: Nam giới ...  \n499996  Nếu mạn tính, cần xác định thời điểm xuất hiện...  \n499997  Thiết bị dùng để lấy bọ ve Dung dịch rửa như d...  \n499998  Đại tràng Sigma bình thưởng trên Siêu âm qua n...  \n499999  Đối tượng - Phương pháp nghiên cứu: Nghiên cứu...  \n\n[500000 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>vi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>To evaluate clinical, subclinical symptoms of ...</td>\n      <td>Nghiên cứu đặc điểm lâm sàng, cận lâm sàng bện...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Evaluate clinical, subclinical symptoms of pat...</td>\n      <td>Đánh giá đặc điểm lâm sàng, cận lâm sàng bệnh ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>There was a relation between vasodilatation an...</td>\n      <td>Có sự liên quan giữa độ quá phát V.a với mức đ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Otittis media effusion on V a is a common dise...</td>\n      <td>Kết luận: Viêm tai ứ dịch trên viêm V.a là bện...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Main symptoms are rhinitis, nasal congestion, ...</td>\n      <td>Triệu chứng cơ năng nổi bật là chảy mũi, ngạt ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>499995</th>\n      <td>Patients and methods: Over-40-year-old men are...</td>\n      <td>Đối tượng và phương pháp nghiên cứu: Nam giới ...</td>\n    </tr>\n    <tr>\n      <th>499996</th>\n      <td>If chronic, the age at onset (eg, since birth,...</td>\n      <td>Nếu mạn tính, cần xác định thời điểm xuất hiện...</td>\n    </tr>\n    <tr>\n      <th>499997</th>\n      <td>Equipment for Removing a Tick Cleansing soluti...</td>\n      <td>Thiết bị dùng để lấy bọ ve Dung dịch rửa như d...</td>\n    </tr>\n    <tr>\n      <th>499998</th>\n      <td>Normal sigmoid at TVUS.</td>\n      <td>Đại tràng Sigma bình thưởng trên Siêu âm qua n...</td>\n    </tr>\n    <tr>\n      <th>499999</th>\n      <td>Method: 19 patients underwent 25 coronectomy p...</td>\n      <td>Đối tượng - Phương pháp nghiên cứu: Nghiên cứu...</td>\n    </tr>\n  </tbody>\n</table>\n<p>500000 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:26.850484Z",
          "iopub.execute_input": "2025-08-14T06:26:26.850716Z",
          "iopub.status.idle": "2025-08-14T06:26:26.858206Z",
          "shell.execute_reply.started": "2025-08-14T06:26:26.850683Z",
          "shell.execute_reply": "2025-08-14T06:26:26.857705Z"
        },
        "id": "Iydd3Y8UkiH-",
        "outputId": "b674951c-5166-403d-e413-8da6a68a5665"
      },
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                     en  \\\n0     Knowledge, practices in public health service ...   \n1     Describe knowledge, practices in public health...   \n2     Methodology: A cross sectional study was used ...   \n3     Results: Percentage of card's holders who knew...   \n4     Percentage of card's holders who went to the f...   \n...                                                 ...   \n2995  Therefore, we conduct research to evaluate the...   \n2996  Methods: A cross-sectional descriptive study w...   \n2997  Results: In 169 patients, 23.1% and 19.5% pati...   \n2998  Self-tanning products do not provide significa...   \n2999  Benzathine penicillin G is a long-acting formu...   \n\n                                                     vi  \n0     Thực trạng kiến thức và thực hành của người có...  \n1     Mô tả thực trạng kiến thức, thực hành của ngườ...  \n2     Phương pháp: Thiết kế nghiên mô tả cắt ngang đ...  \n3     Kết quả: Tỷ lệ người biết được khám chữa bệnh ...  \n4     Tỷ lệ người có thẻ BHYT thực hành khám chữa bệ...  \n...                                                 ...  \n2995  Chính vì vậy chúng tôi tiến hành nghiên cứu đá...  \n2996  Phương pháp nghiên cứu: Nghiên cứu mô tả cắt n...  \n2997  Số lượng bệnh nhân bị di căn hạch rốn phổi cao...  \n2998  Các sản phẩm tự tạo màu da không có khả năng b...  \n2999  Benzathine penicillin G là một dạng thuốc có t...  \n\n[3000 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>vi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Knowledge, practices in public health service ...</td>\n      <td>Thực trạng kiến thức và thực hành của người có...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Describe knowledge, practices in public health...</td>\n      <td>Mô tả thực trạng kiến thức, thực hành của ngườ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Methodology: A cross sectional study was used ...</td>\n      <td>Phương pháp: Thiết kế nghiên mô tả cắt ngang đ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Results: Percentage of card's holders who knew...</td>\n      <td>Kết quả: Tỷ lệ người biết được khám chữa bệnh ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Percentage of card's holders who went to the f...</td>\n      <td>Tỷ lệ người có thẻ BHYT thực hành khám chữa bệ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2995</th>\n      <td>Therefore, we conduct research to evaluate the...</td>\n      <td>Chính vì vậy chúng tôi tiến hành nghiên cứu đá...</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>Methods: A cross-sectional descriptive study w...</td>\n      <td>Phương pháp nghiên cứu: Nghiên cứu mô tả cắt n...</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>Results: In 169 patients, 23.1% and 19.5% pati...</td>\n      <td>Số lượng bệnh nhân bị di căn hạch rốn phổi cao...</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>Self-tanning products do not provide significa...</td>\n      <td>Các sản phẩm tự tạo màu da không có khả năng b...</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>Benzathine penicillin G is a long-acting formu...</td>\n      <td>Benzathine penicillin G là một dạng thuốc có t...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3000 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Chia tập test thành val 20%, test 80%\n",
        "test, val = train_test_split(test, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:26.860444Z",
          "iopub.execute_input": "2025-08-14T06:26:26.860907Z",
          "iopub.status.idle": "2025-08-14T06:26:27.552101Z",
          "shell.execute_reply.started": "2025-08-14T06:26:26.860890Z",
          "shell.execute_reply": "2025-08-14T06:26:27.551554Z"
        },
        "id": "rBHJ-hf3kiH_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:27.552812Z",
          "iopub.execute_input": "2025-08-14T06:26:27.553253Z",
          "iopub.status.idle": "2025-08-14T06:26:27.561194Z",
          "shell.execute_reply.started": "2025-08-14T06:26:27.553233Z",
          "shell.execute_reply": "2025-08-14T06:26:27.560698Z"
        },
        "id": "L4vCYqTEkiIA",
        "outputId": "612f82c1-d31c-4da7-a3a8-02076f19489b"
      },
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                     en  \\\n642   Health related quality of life score and stand...   \n700   Emicizumab is a prophylactic drug for HA, used...   \n226   Policies and regulations on management and tre...   \n1697  Subjects and method: 87 hypertensive patients ...   \n1010  Most soldiers with hearing loss in one ear had...   \n...                                                 ...   \n1638  Results: Overall survival and disease-free sur...   \n1095  Data are retrospective secondary data from med...   \n1130  Recommendation: It is necessary to have soluti...   \n1294  Assessed on BMI, 19% had light weight, 12% wer...   \n860   In children with persistent fever after COVID-...   \n\n                                                     vi  \n642   Điểm HRQoL trung bình và độ lệch chuẩn của toà...  \n700   Emicizumab là thuốc dự phòng xuất huyết trong ...  \n226   Các chính sách, quy định về quản lý và điều tr...  \n1697  Đối tượng và phương pháp: 87 bệnh nhân THA (33...  \n1010  Phần lớn quân nhân nghe kém một tai là nghe ké...  \n...                                                 ...  \n1638  Kết quả: Thời gian sống thêm toàn bộ và sống t...  \n1095   Số liệu được hồi cứu số liệu thứ cấp từ bệnh án.  \n1130  Khuyến nghị: Cần có các giải pháp làm giảm ảnh...  \n1294  Đánh giá theo BMI có 19% gầy nhẹ, 12% gầy vừa,...  \n860   Ở trẻ có biểu hiện sốt kéo dài sau nhiễm COVID...  \n\n[2400 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>vi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>642</th>\n      <td>Health related quality of life score and stand...</td>\n      <td>Điểm HRQoL trung bình và độ lệch chuẩn của toà...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>Emicizumab is a prophylactic drug for HA, used...</td>\n      <td>Emicizumab là thuốc dự phòng xuất huyết trong ...</td>\n    </tr>\n    <tr>\n      <th>226</th>\n      <td>Policies and regulations on management and tre...</td>\n      <td>Các chính sách, quy định về quản lý và điều tr...</td>\n    </tr>\n    <tr>\n      <th>1697</th>\n      <td>Subjects and method: 87 hypertensive patients ...</td>\n      <td>Đối tượng và phương pháp: 87 bệnh nhân THA (33...</td>\n    </tr>\n    <tr>\n      <th>1010</th>\n      <td>Most soldiers with hearing loss in one ear had...</td>\n      <td>Phần lớn quân nhân nghe kém một tai là nghe ké...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1638</th>\n      <td>Results: Overall survival and disease-free sur...</td>\n      <td>Kết quả: Thời gian sống thêm toàn bộ và sống t...</td>\n    </tr>\n    <tr>\n      <th>1095</th>\n      <td>Data are retrospective secondary data from med...</td>\n      <td>Số liệu được hồi cứu số liệu thứ cấp từ bệnh án.</td>\n    </tr>\n    <tr>\n      <th>1130</th>\n      <td>Recommendation: It is necessary to have soluti...</td>\n      <td>Khuyến nghị: Cần có các giải pháp làm giảm ảnh...</td>\n    </tr>\n    <tr>\n      <th>1294</th>\n      <td>Assessed on BMI, 19% had light weight, 12% wer...</td>\n      <td>Đánh giá theo BMI có 19% gầy nhẹ, 12% gầy vừa,...</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>In children with persistent fever after COVID-...</td>\n      <td>Ở trẻ có biểu hiện sốt kéo dài sau nhiễm COVID...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2400 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "val"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:27.561863Z",
          "iopub.execute_input": "2025-08-14T06:26:27.562054Z",
          "iopub.status.idle": "2025-08-14T06:26:27.579237Z",
          "shell.execute_reply.started": "2025-08-14T06:26:27.562040Z",
          "shell.execute_reply": "2025-08-14T06:26:27.578509Z"
        },
        "id": "MMuwhP0TkiIA",
        "outputId": "e01a6743-e181-4a50-f53c-5dc01dd95547"
      },
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                     en  \\\n1801  Dental anxiety was evaluated using the DASS-21...   \n1190  Membrane clotting was the most common complica...   \n1817  However, this procedure hasn't been domestical...   \n251   The general histopathological response was 90.9%.   \n2505  Patients living with HIV infection should be u...   \n...                                                 ...   \n104   Regarding muscle histopathology, the most comm...   \n2087  Thesedays, they are prepared in the product na...   \n599   Hospitals should pay attention to strengthen t...   \n1756   Results: 159 patients participated in the study.   \n1323  Dialysis more than three years and inadequate ...   \n\n                                                     vi  \n1801  Mức độ lo lắng nha khoa được đánh giá theo tha...  \n1190   Đông màng là biến chứng thường gặp nhất (49,3%).  \n1817  Phẫu thuật này hiện còn chưa được phổ biến tro...  \n251            Đáp ứng chung trên mô bệnh học là 90,9%.  \n2505  Những bệnh nhân sống với HIV cần được khuyến k...  \n...                                                 ...  \n104   Về mô bệnh học, các đặc điểm tổn thương thường...  \n2087  Hiện nay, hai dược liệu này được bào chế thành...  \n599   Bệnh viện cần chú ý tăng cường công tác đào tạ...  \n1756    Kết quả: 159 bệnh nhân tham gia vào nghiên cứu.  \n1323  Thời gian lọc máu trên 3 năm và kiểm soát huyế...  \n\n[600 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>vi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1801</th>\n      <td>Dental anxiety was evaluated using the DASS-21...</td>\n      <td>Mức độ lo lắng nha khoa được đánh giá theo tha...</td>\n    </tr>\n    <tr>\n      <th>1190</th>\n      <td>Membrane clotting was the most common complica...</td>\n      <td>Đông màng là biến chứng thường gặp nhất (49,3%).</td>\n    </tr>\n    <tr>\n      <th>1817</th>\n      <td>However, this procedure hasn't been domestical...</td>\n      <td>Phẫu thuật này hiện còn chưa được phổ biến tro...</td>\n    </tr>\n    <tr>\n      <th>251</th>\n      <td>The general histopathological response was 90.9%.</td>\n      <td>Đáp ứng chung trên mô bệnh học là 90,9%.</td>\n    </tr>\n    <tr>\n      <th>2505</th>\n      <td>Patients living with HIV infection should be u...</td>\n      <td>Những bệnh nhân sống với HIV cần được khuyến k...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>Regarding muscle histopathology, the most comm...</td>\n      <td>Về mô bệnh học, các đặc điểm tổn thương thường...</td>\n    </tr>\n    <tr>\n      <th>2087</th>\n      <td>Thesedays, they are prepared in the product na...</td>\n      <td>Hiện nay, hai dược liệu này được bào chế thành...</td>\n    </tr>\n    <tr>\n      <th>599</th>\n      <td>Hospitals should pay attention to strengthen t...</td>\n      <td>Bệnh viện cần chú ý tăng cường công tác đào tạ...</td>\n    </tr>\n    <tr>\n      <th>1756</th>\n      <td>Results: 159 patients participated in the study.</td>\n      <td>Kết quả: 159 bệnh nhân tham gia vào nghiên cứu.</td>\n    </tr>\n    <tr>\n      <th>1323</th>\n      <td>Dialysis more than three years and inadequate ...</td>\n      <td>Thời gian lọc máu trên 3 năm và kiểm soát huyế...</td>\n    </tr>\n  </tbody>\n</table>\n<p>600 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def english_preprocessing(text):\n",
        "    # Chuyển chữ hoa thành chữ thường\n",
        "    text = text.lower()\n",
        "\n",
        "    # Chuẩn hóa khoảng trắng ban đầu\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "    # Tách dấu hai chấm nếu dính liền (e.g., methods:This → methods: This)\n",
        "    text = re.sub(r'(?<=\\w):(?=\\w)', ': ', text)\n",
        "\n",
        "    # Xóa dấu ngoặc kép không cần thiết, nhưng giữ lại dấu nháy đơn trong từ (e.g., it's, don't)\n",
        "    text = re.sub(r'[“”\\\"`]', '', text)\n",
        "\n",
        "    # Xử lý đơn vị viết dính (e.g., 25ui/l → 25 ui/l)\n",
        "    text = re.sub(r'(\\d+)\\s*([a-zA-Z]+)', r'\\1 \\2', text)\n",
        "\n",
        "    # Chuẩn hóa số: \"81, 3%\" → \"81.3%\", \"9, 001\" → \"9001\"\n",
        "    text = re.sub(r'(\\d),\\s*(\\d)', r'\\1.\\2', text)  # 81, 3 → 81.3\n",
        "    text = re.sub(r'(?<=\\d)\\s*,\\s*(?=\\d)', '', text)  # 9, 001 → 9001\n",
        "\n",
        "    # Chuẩn hóa các loại dash\n",
        "    text = re.sub(r'[–—−]', '-', text)\n",
        "\n",
        "    # Giữ lại định dạng đúng cho các ký hiệu y học\n",
        "    text = re.sub(r'\\s*/\\s*', '/', text)   # PET / CT → PET/CT\n",
        "    text = re.sub(r'\\s*\\+\\s*', '+', text)  # ( + ) → (+)\n",
        "\n",
        "    # Thêm khoảng trắng quanh toán tử\n",
        "    text = re.sub(r'\\s*(<=|>=|=|≠|±|<|>)\\s*', r' \\1 ', text)\n",
        "\n",
        "    # Tách số và dấu %\n",
        "    text = re.sub(r'(\\d+(\\.\\d+)?)%', r'\\1 %', text)\n",
        "\n",
        "    # Làm sạch cuối cùng\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:27.579987Z",
          "iopub.execute_input": "2025-08-14T06:26:27.580291Z",
          "iopub.status.idle": "2025-08-14T06:26:27.595357Z",
          "shell.execute_reply.started": "2025-08-14T06:26:27.580274Z",
          "shell.execute_reply": "2025-08-14T06:26:27.594704Z"
        },
        "id": "GcfDK-1TkiIA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def vietnamese_preprocessing(text):\n",
        "    # Chuyển chữ hoa thành chữ thường\n",
        "    text = text.lower()\n",
        "\n",
        "    # Chuẩn hóa khoảng trắng ban đầu\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "    # Tách dấu hai chấm nếu dính liền (e.g., methods:This → methods: This)\n",
        "    text = re.sub(r'(?<=\\w):(?=\\w)', ': ', text)\n",
        "\n",
        "    # Loại bỏ dấu câu không cần thiết\n",
        "    text = re.sub(r'[“”\\\"\\'`]', '', text)\n",
        "\n",
        "    # Xử lý đơn vị viết dính (e.g., 25ui/l → 25 ui/l)\n",
        "    text = re.sub(r'(\\d+)\\s*([a-zA-Z]+)', r'\\1 \\2', text)\n",
        "\n",
        "    # Chuẩn hóa số: \"81, 3%\" → \"81.3%\", \"9, 001\" → \"9001\"\n",
        "    text = re.sub(r'(\\d),\\s*(\\d)', r'\\1.\\2', text)  # 81, 3 → 81.3\n",
        "    text = re.sub(r'(?<=\\d)\\s*,\\s*(?=\\d)', '', text)  # 9, 001 → 9001\n",
        "\n",
        "    # Chuẩn hóa các loại dash\n",
        "    text = re.sub(r'[–—−]', '-', text)\n",
        "\n",
        "    # Giữ lại định dạng đúng cho các ký hiệu y học\n",
        "    text = re.sub(r'\\s*/\\s*', '/', text)   # PET / CT → PET/CT\n",
        "    text = re.sub(r'\\s*\\+\\s*', '+', text)  # ( + ) → (+)\n",
        "\n",
        "    # Thêm khoảng trắng quanh toán tử\n",
        "    text = re.sub(r'\\s*(<=|>=|=|≠|±|<|>)\\s*', r' \\1 ', text)\n",
        "\n",
        "    # Tách số và dấu %\n",
        "    text = re.sub(r'(\\d+(\\.\\d+)?)%', r'\\1 %', text)\n",
        "\n",
        "    # Làm sạch cuối cùng\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:27.595996Z",
          "iopub.execute_input": "2025-08-14T06:26:27.596202Z",
          "iopub.status.idle": "2025-08-14T06:26:27.608698Z",
          "shell.execute_reply.started": "2025-08-14T06:26:27.596187Z",
          "shell.execute_reply": "2025-08-14T06:26:27.608190Z"
        },
        "id": "pTlpppuFkiIB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train['en'] = train['en'].apply(english_preprocessing)\n",
        "train['vi'] = train['vi'].apply(vietnamese_preprocessing)\n",
        "\n",
        "test['en'] = test['en'].apply(english_preprocessing)\n",
        "test['vi'] = test['vi'].apply(vietnamese_preprocessing)\n",
        "\n",
        "val['en'] = val['en'].apply(english_preprocessing)\n",
        "val['vi'] = val['vi'].apply(vietnamese_preprocessing)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:26:27.609306Z",
          "iopub.execute_input": "2025-08-14T06:26:27.609499Z",
          "iopub.status.idle": "2025-08-14T06:27:31.741858Z",
          "shell.execute_reply.started": "2025-08-14T06:26:27.609485Z",
          "shell.execute_reply": "2025-08-14T06:27:31.741326Z"
        },
        "id": "BrC6DrIXkiIB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# English tokenizer\n",
        "tokenizer_en = ByteLevelBPETokenizer()\n",
        "tokenizer_en.train_from_iterator(\n",
        "    train['en'].tolist(),\n",
        "    vocab_size=32000,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n",
        ")\n",
        "tokenizer_en.save(\"tokenizer_en.json\")\n",
        "\n",
        "# Vietnamese tokenizer\n",
        "tokenizer_vi = ByteLevelBPETokenizer()\n",
        "tokenizer_vi.train_from_iterator(\n",
        "    train['vi'].tolist(),\n",
        "    vocab_size=32000,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n",
        ")\n",
        "tokenizer_vi.save(\"tokenizer_vi.json\")"
      ],
      "metadata": {
        "id": "djEE9TEOkrWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "tokenizer_en = Tokenizer.from_file(\"/kaggle/input/tokenizer/tokenizer_en.json\")\n",
        "tokenizer_vi = Tokenizer.from_file(\"/kaggle/input/tokenizer/tokenizer_vi.json\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:27:31.742545Z",
          "iopub.execute_input": "2025-08-14T06:27:31.742745Z",
          "iopub.status.idle": "2025-08-14T06:27:32.084413Z",
          "shell.execute_reply.started": "2025-08-14T06:27:31.742728Z",
          "shell.execute_reply": "2025-08-14T06:27:32.083809Z"
        },
        "id": "0gnFTaAvkiIB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 256  # giới hạn chiều dài token sequence\n",
        "\n",
        "def encode_dataset(df, tokenizer_en, tokenizer_vi):\n",
        "    bos_en = tokenizer_en.token_to_id(\"<s>\")\n",
        "    eos_en = tokenizer_en.token_to_id(\"</s>\")\n",
        "    bos_vi = tokenizer_vi.token_to_id(\"<s>\")\n",
        "    eos_vi = tokenizer_vi.token_to_id(\"</s>\")\n",
        "\n",
        "    def truncate(seq, max_len):\n",
        "        return seq[:max_len] if len(seq) > max_len else seq\n",
        "\n",
        "    def encode_pair(en_text, vi_text):\n",
        "        # Encode English\n",
        "        en_ids = tokenizer_en.encode(en_text).ids\n",
        "        en_ids = truncate(en_ids, MAX_LEN - 2)  # trừ 2 vì thêm <s> và </s>\n",
        "        en_input = [bos_en] + en_ids + [eos_en]\n",
        "\n",
        "        # Encode Vietnamese (một lần)\n",
        "        vi_ids = tokenizer_vi.encode(vi_text).ids\n",
        "        vi_ids = truncate(vi_ids, MAX_LEN - 2)  # trừ 2 vì thêm <s> và </s>\n",
        "        vi_full = [bos_vi] + vi_ids + [eos_vi]\n",
        "\n",
        "        # Tách ra:\n",
        "        vi_input = vi_full[:-1]  # bỏ </s>\n",
        "        vi_target = vi_full[1:]  # bỏ <s>\n",
        "\n",
        "        return en_input, vi_input, vi_target\n",
        "\n",
        "    encoded = df.apply(lambda row: encode_pair(row['en'], row['vi']), axis=1)\n",
        "    en_input, vi_input, vi_target = zip(*encoded)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'en_input': en_input,\n",
        "        'vi_input': vi_input,\n",
        "        'vi_target': vi_target\n",
        "    })"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:27:32.085054Z",
          "iopub.execute_input": "2025-08-14T06:27:32.085261Z",
          "iopub.status.idle": "2025-08-14T06:27:32.092100Z",
          "shell.execute_reply.started": "2025-08-14T06:27:32.085244Z",
          "shell.execute_reply": "2025-08-14T06:27:32.091257Z"
        },
        "id": "REMRnKIhkiIB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoded = encode_dataset(train, tokenizer_en, tokenizer_vi)\n",
        "test_encoded = encode_dataset(test, tokenizer_en, tokenizer_vi)\n",
        "val_encoded = encode_dataset(val, tokenizer_en, tokenizer_vi)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:27:32.092889Z",
          "iopub.execute_input": "2025-08-14T06:27:32.093189Z",
          "iopub.status.idle": "2025-08-14T06:29:22.311599Z",
          "shell.execute_reply.started": "2025-08-14T06:27:32.093160Z",
          "shell.execute_reply": "2025-08-14T06:29:22.310943Z"
        },
        "id": "nidEjJSrkiIB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer architecture"
      ],
      "metadata": {
        "id": "-RuMsHIzkiIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "    super(InputEmbeddings, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "    # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.312320Z",
          "iopub.execute_input": "2025-08-14T06:29:22.312580Z",
          "iopub.status.idle": "2025-08-14T06:29:22.317302Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.312558Z",
          "shell.execute_reply": "2025-08-14T06:29:22.316537Z"
        },
        "id": "ZECvFv94kiIC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Create a matrix of shape (seq_len, d_model)\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "    # Create a vector of shape (seq_len)\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "    # Create a vector of shape (d_model)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
        "    # Apply sine to even indices\n",
        "    pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
        "    # Apply cosine to odd indices\n",
        "    pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
        "    # Add a batch dimension to the positional encoding\n",
        "    pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "    # Register the positional encoding as a buffer\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.318244Z",
          "iopub.execute_input": "2025-08-14T06:29:22.319013Z",
          "iopub.status.idle": "2025-08-14T06:29:22.339358Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.318981Z",
          "shell.execute_reply": "2025-08-14T06:29:22.338717Z"
        },
        "id": "kq5qgyLPkiIC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, h: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model # Embedding vector size\n",
        "    self.h = h # Number of heads\n",
        "\n",
        "    self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "      attention_scores = attention_scores.masked_fill(mask == 0, torch.finfo(attention_scores.dtype).min)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "    return torch.matmul(attention_scores, value), attention_scores\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "    # q, k, v: (batch, seq_len, d_model)\n",
        "    # mask: (batch, seq_len, seq_len)\n",
        "    query = self.w_q(q)\n",
        "    key = self.w_k(k)\n",
        "    value = self.w_v(v)\n",
        "\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "    # Calculate attention using function we will define next\n",
        "    x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    # Combine all the heads together\n",
        "    # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "    # Apply one final linear transformation\n",
        "    return self.w_o(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.340062Z",
          "iopub.execute_input": "2025-08-14T06:29:22.340231Z",
          "iopub.status.idle": "2025-08-14T06:29:22.358800Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.340217Z",
          "shell.execute_reply": "2025-08-14T06:29:22.358018Z"
        },
        "id": "zJMv-3-CkiID"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.359535Z",
          "iopub.execute_input": "2025-08-14T06:29:22.359850Z",
          "iopub.status.idle": "2025-08-14T06:29:22.378951Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.359830Z",
          "shell.execute_reply": "2025-08-14T06:29:22.378336Z"
        },
        "id": "IwJLp_PzkiID"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, features: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = nn.LayerNorm(features)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    # Apply residual connection\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.379642Z",
          "iopub.execute_input": "2025-08-14T06:29:22.379894Z",
          "iopub.status.idle": "2025-08-14T06:29:22.396490Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.379871Z",
          "shell.execute_reply": "2025-08-14T06:29:22.395923Z"
        },
        "id": "5Sc-VlbpkiID"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, features: int,\n",
        "                 self_attention_block1: MultiHeadAttentionBlock,\n",
        "                 self_attention_block2: MultiHeadAttentionBlock,\n",
        "                 feed_forward_block: FeedForwardBlock,\n",
        "                 dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention_block1 = self_attention_block1\n",
        "        self.self_attention_block2 = self_attention_block2\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        # 3 residual connections: SA1, SA2, FFN\n",
        "        self.residual_connections = nn.ModuleList([\n",
        "            ResidualConnection(features, dropout),\n",
        "            ResidualConnection(features, dropout),\n",
        "            ResidualConnection(features, dropout)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # First self-attention\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block1(x, x, x, src_mask))\n",
        "        # Second self-attention\n",
        "        x = self.residual_connections[1](x, lambda x: self.self_attention_block2(x, x, x, src_mask))\n",
        "        # Feed-forward\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.397338Z",
          "iopub.execute_input": "2025-08-14T06:29:22.397604Z",
          "iopub.status.idle": "2025-08-14T06:29:22.419042Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.397582Z",
          "shell.execute_reply": "2025-08-14T06:29:22.418504Z"
        },
        "id": "o_QuK_L5kiIE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = nn.LayerNorm(features)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.422685Z",
          "iopub.execute_input": "2025-08-14T06:29:22.422879Z",
          "iopub.status.idle": "2025-08-14T06:29:22.435855Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.422859Z",
          "shell.execute_reply": "2025-08-14T06:29:22.435268Z"
        },
        "id": "uYHcC_IUkiIE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "    x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "    x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "    return x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.436562Z",
          "iopub.execute_input": "2025-08-14T06:29:22.436733Z",
          "iopub.status.idle": "2025-08-14T06:29:22.454890Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.436719Z",
          "shell.execute_reply": "2025-08-14T06:29:22.454299Z"
        },
        "id": "4C_IpWo8kiIE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = nn.LayerNorm(features)\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.455591Z",
          "iopub.execute_input": "2025-08-14T06:29:22.455842Z",
          "iopub.status.idle": "2025-08-14T06:29:22.476920Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.455821Z",
          "shell.execute_reply": "2025-08-14T06:29:22.476276Z"
        },
        "id": "Cqgg_H89kiIE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "    return self.proj(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.477763Z",
          "iopub.execute_input": "2025-08-14T06:29:22.477967Z",
          "iopub.status.idle": "2025-08-14T06:29:22.494675Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.477951Z",
          "shell.execute_reply": "2025-08-14T06:29:22.494087Z"
        },
        "id": "kWsql8uUkiIE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.src_pos = src_pos\n",
        "    self.tgt_pos = tgt_pos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "    d_model = src_embed.d_model\n",
        "    self.src_pos_norm = nn.LayerNorm(d_model)\n",
        "    self.tgt_pos_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src = self.src_embed(src)\n",
        "    src = self.src_pos(src)\n",
        "    src = self.src_pos_norm(src)\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "  def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "    tgt = self.tgt_embed(tgt)\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    tgt = self.tgt_pos_norm(tgt)\n",
        "    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    return self.projection_layer(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.495315Z",
          "iopub.execute_input": "2025-08-14T06:29:22.495546Z",
          "iopub.status.idle": "2025-08-14T06:29:22.513891Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.495521Z",
          "shell.execute_reply": "2025-08-14T06:29:22.513245Z"
        },
        "id": "s83DPQgAkiIF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block_1 = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        encoder_self_attention_block_2 = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderLayer(d_model, encoder_self_attention_block_1, encoder_self_attention_block_2, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N - 2):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderLayer(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "    # weight typing\n",
        "    projection_layer.proj.weight = tgt_embed.embedding.weight\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.514624Z",
          "iopub.execute_input": "2025-08-14T06:29:22.514885Z",
          "iopub.status.idle": "2025-08-14T06:29:22.537574Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.514869Z",
          "shell.execute_reply": "2025-08-14T06:29:22.536809Z"
        },
        "id": "yE1TOOb6kiIF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Process"
      ],
      "metadata": {
        "id": "rWhsjoQOwJg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tqdm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.538400Z",
          "iopub.execute_input": "2025-08-14T06:29:22.538614Z",
          "iopub.status.idle": "2025-08-14T06:29:22.560640Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.538600Z",
          "shell.execute_reply": "2025-08-14T06:29:22.559996Z"
        },
        "id": "12Fziqx6kiIF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.src = df['en_input']\n",
        "        self.tgt_in = df['vi_input']\n",
        "        self.tgt_out = df['vi_target']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Chỉ trả về tensor, không padding\n",
        "        return {\n",
        "            'src': torch.tensor(self.src.iloc[idx], dtype=torch.long),\n",
        "            'tgt_in': torch.tensor(self.tgt_in.iloc[idx], dtype=torch.long),\n",
        "            'tgt_out': torch.tensor(self.tgt_out.iloc[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Hàm collate_fn\n",
        "def collate_fn(batch):\n",
        "    pad_id_en = tokenizer_en.token_to_id(\"<pad>\")\n",
        "    pad_id_vi = tokenizer_vi.token_to_id(\"<pad>\")\n",
        "\n",
        "    src_batch = [item['src'] for item in batch]\n",
        "    tgt_in_batch = [item['tgt_in'] for item in batch]\n",
        "    tgt_out_batch = [item['tgt_out'] for item in batch]\n",
        "\n",
        "    # pad_sequence sẽ tự động pad đến độ dài lớn nhất trong batch\n",
        "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=pad_id_en)\n",
        "    tgt_in_padded = pad_sequence(tgt_in_batch, batch_first=True, padding_value=pad_id_vi)\n",
        "    tgt_out_padded = pad_sequence(tgt_out_batch, batch_first=True, padding_value=pad_id_vi)\n",
        "\n",
        "    return {\n",
        "        'src': src_padded,\n",
        "        'tgt_in': tgt_in_padded,\n",
        "        'tgt_out': tgt_out_padded\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.561491Z",
          "iopub.execute_input": "2025-08-14T06:29:22.561664Z",
          "iopub.status.idle": "2025-08-14T06:29:22.578515Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.561651Z",
          "shell.execute_reply": "2025-08-14T06:29:22.577778Z"
        },
        "id": "n2Hu9fgZkiIF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TranslationDataset(train_encoded)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "test_dataset = TranslationDataset(test_encoded)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "val_dataset = TranslationDataset(val_encoded)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.579252Z",
          "iopub.execute_input": "2025-08-14T06:29:22.579479Z",
          "iopub.status.idle": "2025-08-14T06:29:22.599499Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.579451Z",
          "shell.execute_reply": "2025-08-14T06:29:22.598716Z"
        },
        "id": "690esq59kiIG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm tạo mask\n",
        "def create_mask(src, tgt, pad_id=tokenizer_en.token_to_id(\"<pad>\")):\n",
        "    # src: (batch, src_len), tgt: (batch, tgt_len)\n",
        "    src_mask = (src != pad_id).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, src_len)\n",
        "    tgt_pad_mask = (tgt != pad_id).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, tgt_len)\n",
        "    tgt_len = tgt.size(1)\n",
        "    tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()  # (tgt_len, tgt_len)\n",
        "    tgt_mask = tgt_pad_mask & tgt_sub_mask  # (batch, 1, tgt_len, tgt_len)\n",
        "    return src_mask, tgt_mask"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.600334Z",
          "iopub.execute_input": "2025-08-14T06:29:22.600579Z",
          "iopub.status.idle": "2025-08-14T06:29:22.615010Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.600554Z",
          "shell.execute_reply": "2025-08-14T06:29:22.614406Z"
        },
        "id": "QcTzKzWvkiIG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, loss_fn, device, scheduler=None):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        src = batch['src'].to(device)\n",
        "        tgt_input = batch['tgt_in'].to(device)\n",
        "        tgt_output = batch['tgt_out'].to(device)\n",
        "\n",
        "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(device_type=device.type):  # mixed precision context\n",
        "            encoder_output = model.encode(src, src_mask)\n",
        "            decoder_output = model.decode(encoder_output, src_mask, tgt_input, tgt_mask)\n",
        "            output = model.project(decoder_output)\n",
        "\n",
        "            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
        "\n",
        "        # backward and step using scaler\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.615840Z",
          "iopub.execute_input": "2025-08-14T06:29:22.616115Z",
          "iopub.status.idle": "2025-08-14T06:29:22.732770Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.616082Z",
          "shell.execute_reply": "2025-08-14T06:29:22.732063Z"
        },
        "id": "SueBv8_JkiIG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_one_epoch(model, dataloader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        src = batch['src'].to(device)\n",
        "        tgt_input = batch['tgt_in'].to(device)\n",
        "        tgt_output = batch['tgt_out'].to(device)\n",
        "\n",
        "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        encoder_output = model.encode(src, src_mask)\n",
        "        decoder_output = model.decode(encoder_output, src_mask, tgt_input, tgt_mask)\n",
        "        output = model.project(decoder_output)\n",
        "\n",
        "        loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(val_loss=loss.item())\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.733667Z",
          "iopub.execute_input": "2025-08-14T06:29:22.733932Z",
          "iopub.status.idle": "2025-08-14T06:29:22.751434Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.733909Z",
          "shell.execute_reply": "2025-08-14T06:29:22.750688Z"
        },
        "id": "IiQz0itLkiIH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupInverseSquareRootScheduler:\n",
        "    def __init__(self, optimizer, d_model, warmup_steps):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.d_model = d_model\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        lr = self._get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    def _get_lr(self):\n",
        "        arg1 = self.step_num ** (-0.5)\n",
        "        arg2 = self.step_num * (self.warmup_steps ** (-1.5))\n",
        "        return (self.d_model ** -0.5) * min(arg1, arg2)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.752138Z",
          "iopub.execute_input": "2025-08-14T06:29:22.752402Z",
          "iopub.status.idle": "2025-08-14T06:29:22.771779Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.752384Z",
          "shell.execute_reply": "2025-08-14T06:29:22.771051Z"
        },
        "id": "Cw7YxneOkiIH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "src_vocab_size = tokenizer_en.get_vocab_size()\n",
        "tgt_vocab_size = tokenizer_vi.get_vocab_size()\n",
        "\n",
        "model = build_transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    src_seq_len=256,\n",
        "    tgt_seq_len=256,\n",
        "    d_model=512,\n",
        "    N=6,\n",
        "    h=8,\n",
        "    dropout=0.1,\n",
        "    d_ff=1024\n",
        ").to(device)\n",
        "\n",
        "# Loss with label smoothing\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_vi.token_to_id(\"<pad>\"), label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Tạo Scheduler\n",
        "scheduler = WarmupInverseSquareRootScheduler(optimizer, d_model=512, warmup_steps=4000)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:22.772678Z",
          "iopub.execute_input": "2025-08-14T06:29:22.772880Z",
          "iopub.status.idle": "2025-08-14T06:29:27.684384Z",
          "shell.execute_reply.started": "2025-08-14T06:29:22.772864Z",
          "shell.execute_reply": "2025-08-14T06:29:27.683735Z"
        },
        "id": "B0ylGh5_kiIH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(size):\n",
        "    \"\"\"\n",
        "    Tạo causal mask: chỉ cho phép mỗi vị trí nhìn thấy các token trước đó (kể cả chính nó).\n",
        "    Output shape: (1, 1, size, size)\n",
        "    \"\"\"\n",
        "    return torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(1)  # (1, 1, size, size)\n",
        "\n",
        "def beam_search_decode_batch_parallel(\n",
        "    model, src, src_mask, tokenizer_vi, max_len, device, beam_size=4, alpha=0.6\n",
        "):\n",
        "    def get_token_id(tok, fallback=None):\n",
        "        tid = tokenizer_vi.token_to_id(tok)\n",
        "        if tid is None and fallback is not None:\n",
        "            tid = tokenizer_vi.token_to_id(fallback)\n",
        "        return tid\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        sos_id = get_token_id(\"<s>\", \"[SOS]\")\n",
        "        eos_id = get_token_id(\"</s>\", \"[EOS]\")\n",
        "        pad_id = get_token_id(\"<pad>\", \"[PAD]\")\n",
        "\n",
        "        if sos_id is None or eos_id is None or pad_id is None:\n",
        "            raise ValueError(f\"Special tokens missing: sos={sos_id}, eos={eos_id}, pad={pad_id}\")\n",
        "\n",
        "        def length_penalty_fn(length, alpha):\n",
        "            return ((5.0 + length) ** alpha) / ((5.0 + 1.0) ** alpha)\n",
        "\n",
        "        # Encode input\n",
        "        memory = model.encode(src, src_mask)\n",
        "        memory = memory.repeat_interleave(beam_size, dim=0)\n",
        "        src_mask = src_mask.repeat_interleave(beam_size, dim=0)\n",
        "\n",
        "        seqs = torch.full((batch_size * beam_size, 1), sos_id, dtype=torch.long, device=device)\n",
        "        scores = torch.zeros(batch_size * beam_size, device=device)\n",
        "        finished_flags = torch.zeros(batch_size * beam_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        for step in range(1, max_len):\n",
        "            tgt_mask = causal_mask(seqs.size(1)).to(device)\n",
        "            dec_out = model.decode(memory, src_mask, seqs, tgt_mask)\n",
        "            logits = model.project(dec_out[:, -1, :])\n",
        "            log_probs = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "            # Beam đã kết thúc chỉ sinh <pad>\n",
        "            log_probs[finished_flags] = -1e9\n",
        "            log_probs[finished_flags, pad_id] = 0\n",
        "\n",
        "            next_scores, next_tokens = torch.topk(log_probs, beam_size, dim=-1)\n",
        "\n",
        "            # Reshape để chọn top beam cho batch\n",
        "            next_scores = next_scores.view(batch_size, beam_size, beam_size)\n",
        "            next_tokens = next_tokens.view(batch_size, beam_size, beam_size)\n",
        "\n",
        "            # Tính total score với length penalty sớm\n",
        "            total_scores = scores.view(batch_size, beam_size, 1) + next_scores\n",
        "            lp = length_penalty_fn(step + 1, alpha)\n",
        "            total_scores = total_scores / lp\n",
        "\n",
        "            # Chọn top beam\n",
        "            top_scores, top_indices = torch.topk(total_scores.view(batch_size, -1), beam_size, dim=-1)\n",
        "            beam_indices = top_indices // beam_size\n",
        "            token_indices = top_indices % beam_size\n",
        "\n",
        "            # Vector hóa update sequences\n",
        "            old_beam_ids = (beam_indices + torch.arange(batch_size, device=device).unsqueeze(1) * beam_size).view(-1)\n",
        "            chosen_tokens = next_tokens[torch.arange(batch_size, device=device).unsqueeze(1), beam_indices, token_indices].view(-1)\n",
        "\n",
        "            seqs = torch.cat([seqs[old_beam_ids], chosen_tokens.unsqueeze(1)], dim=-1)\n",
        "            scores = top_scores.view(-1)\n",
        "\n",
        "            # Cập nhật finished flags\n",
        "            finished_flags = finished_flags[old_beam_ids] | (chosen_tokens == eos_id)\n",
        "\n",
        "            if finished_flags.all():\n",
        "                break\n",
        "\n",
        "        # Chọn best beam cuối cùng\n",
        "        final_seqs = []\n",
        "        for b in range(batch_size):\n",
        "            start = b * beam_size\n",
        "            end = start + beam_size\n",
        "            cand_scores = scores[start:end]\n",
        "            cand_seqs = seqs[start:end]\n",
        "            lengths = torch.tensor([len(s) for s in cand_seqs], dtype=torch.float, device=device)\n",
        "            lp = length_penalty_fn(lengths, alpha)\n",
        "            best_idx = torch.argmax(cand_scores / lp).item()\n",
        "            final_seqs.append(cand_seqs[best_idx])\n",
        "\n",
        "        # Pad output\n",
        "        max_len_final = max(len(s) for s in final_seqs)\n",
        "        padded = torch.full((batch_size, max_len_final), pad_id, dtype=torch.long, device=device)\n",
        "        for i, seq in enumerate(final_seqs):\n",
        "            padded[i, :len(seq)] = seq\n",
        "\n",
        "        return padded"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:27.685069Z",
          "iopub.execute_input": "2025-08-14T06:29:27.685418Z",
          "iopub.status.idle": "2025-08-14T06:29:27.699997Z",
          "shell.execute_reply.started": "2025-08-14T06:29:27.685400Z",
          "shell.execute_reply": "2025-08-14T06:29:27.699123Z"
        },
        "id": "2uEDIDpqkiII"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "def evaluate_bleu(model, dataloader, tokenizer_en, tokenizer_vi, device, max_len=128):\n",
        "    model.eval()\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    sos_id = tokenizer_vi.token_to_id(\"[SOS]\") or tokenizer_vi.token_to_id(\"<s>\")\n",
        "    eos_id = tokenizer_vi.token_to_id(\"[EOS]\") or tokenizer_vi.token_to_id(\"</s>\")\n",
        "    pad_id = tokenizer_vi.token_to_id(\"<pad>\")\n",
        "    pad_src_id = tokenizer_en.token_to_id(\"<pad>\")\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating BLEU\"):\n",
        "        src = batch['src'].to(device)\n",
        "        tgt_input = batch['tgt_in'].to(device)\n",
        "        tgt_output = batch['tgt_out'].to(device)\n",
        "\n",
        "        src_mask = (src != pad_src_id).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        pred_ids_batch = beam_search_decode_batch_parallel(model, src, src_mask, tokenizer_vi, max_len, device, beam_size=4)\n",
        "\n",
        "        for pred_ids, ref_ids in zip(pred_ids_batch, tgt_output):\n",
        "            pred_tokens = [\n",
        "                tokenizer_vi.id_to_token(id.item())\n",
        "                for id in pred_ids\n",
        "                if id.item() not in {sos_id, eos_id, pad_id}\n",
        "            ]\n",
        "\n",
        "            ref_tokens = [\n",
        "                tokenizer_vi.id_to_token(id.item())\n",
        "                for id in ref_ids\n",
        "                if id.item() not in {sos_id, eos_id, pad_id}\n",
        "            ]\n",
        "\n",
        "            hypotheses.append(pred_tokens)\n",
        "            references.append([ref_tokens])  # each reference must be a list of lists\n",
        "\n",
        "    bleu = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
        "\n",
        "    print(f\"BLEU score (nltk): {bleu * 100:.2f}\")\n",
        "    return bleu"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:27.700802Z",
          "iopub.execute_input": "2025-08-14T06:29:27.701049Z",
          "iopub.status.idle": "2025-08-14T06:29:28.624122Z",
          "shell.execute_reply.started": "2025-08-14T06:29:27.701034Z",
          "shell.execute_reply": "2025-08-14T06:29:28.623511Z"
        },
        "id": "92fqTxy7kiII"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load lại model\n",
        "model = build_transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    src_seq_len=256,\n",
        "    tgt_seq_len=256,\n",
        "    d_model=512,\n",
        "    N=6,\n",
        "    h=8,\n",
        "    dropout=0.1,\n",
        "    d_ff=1024\n",
        ").to(device)\n",
        "\n",
        "checkpoint_path = '/kaggle/input/model-trained/re_transformer_best.pth'\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)  # Tạo lại optimizer tương ứng\n",
        "scaler = GradScaler()  # ✅ Đặt ở đây sau khi tạo model + optimizer\n",
        "\n",
        "scheduler = WarmupInverseSquareRootScheduler(optimizer, d_model=512, warmup_steps=4000)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:28.624865Z",
          "iopub.execute_input": "2025-08-14T06:29:28.625069Z",
          "iopub.status.idle": "2025-08-14T06:29:32.093732Z",
          "shell.execute_reply.started": "2025-08-14T06:29:28.625052Z",
          "shell.execute_reply": "2025-08-14T06:29:32.092989Z"
        },
        "id": "lgWxpGw5kiII"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = 2.2\n",
        "early_stop_counter = 0\n",
        "patience = 5\n",
        "\n",
        "for epoch in range(15):\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    train_loss = train_one_epoch(model, train_dataloader, optimizer, loss_fn, device, scheduler=scheduler)\n",
        "    val_loss = evaluate_one_epoch(model, val_dataloader, loss_fn, device)\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "    scheduler.step()\n",
        "\n",
        "    # Kiểm tra cải thiện\n",
        "    improved = False\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        improved = True\n",
        "\n",
        "    if improved:\n",
        "        early_stop_counter = 0\n",
        "        save_path = '/kaggle/working/re_transformer_best.pth'\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(\"Saved model!\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        print(f\"Marked {early_stop_counter}/5\")\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"No improvement after several epochs. Early stopping.\")\n",
        "            break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T06:29:32.094881Z",
          "iopub.execute_input": "2025-08-14T06:29:32.095172Z",
          "execution_failed": "2025-08-14T09:22:36.085Z"
        },
        "id": "GxCoMo5YkiIN",
        "outputId": "2354558e-e1d6-4b8e-c295-22b9be890195"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 2.1234 | Val Loss: 2.2070\nMarked 1/5\nEpoch 2\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 2.0630 | Val Loss: 2.1813\nSaved model!\nEpoch 3\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 1.9758 | Val Loss: 2.1622\nSaved model!\nEpoch 4\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training:  95%|█████████▌| 14850/15625 [41:21<02:11,  5.91it/s, loss=1.94] ",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}