{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12731200,
          "sourceType": "datasetVersion",
          "datasetId": 8047094
        },
        {
          "sourceId": 12740416,
          "sourceType": "datasetVersion",
          "datasetId": 8053446
        },
        {
          "sourceId": 12745010,
          "sourceType": "datasetVersion",
          "datasetId": 8053449
        },
        {
          "sourceId": 12757390,
          "sourceType": "datasetVersion",
          "datasetId": 8053456
        },
        {
          "sourceId": 12761534,
          "sourceType": "datasetVersion",
          "datasetId": 8047110
        },
        {
          "sourceId": 12731209,
          "sourceType": "datasetVersion",
          "datasetId": 8047102
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBkwwM9mPpAD",
        "outputId": "a190e4d9-56b4-4e82-c1da-fa406cfcadb5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:23:00.534926Z",
          "iopub.execute_input": "2025-08-14T09:23:00.535606Z",
          "iopub.status.idle": "2025-08-14T09:23:04.802105Z",
          "shell.execute_reply.started": "2025-08-14T09:23:00.535570Z",
          "shell.execute_reply": "2025-08-14T09:23:04.801352Z"
        },
        "id": "f_7KL2L8PH6O"
      },
      "outputs": [],
      "execution_count": 48
    },
    {
      "cell_type": "code",
      "source": [
        "en_train_file = '/content/drive/MyDrive/Released Corpus/train.en.txt'\n",
        "vi_train_file = '/content/drive/MyDrive/Released Corpus/train.vi.txt'\n",
        "\n",
        "# Read lines\n",
        "with open(en_train_file, 'r', encoding='utf-8') as f_train_en:\n",
        "    en_train_lines = [line.strip() for line in f_train_en.readlines()]\n",
        "\n",
        "with open(vi_train_file, 'r', encoding='utf-8') as f_train_vi:\n",
        "    vi_train_lines = [line.strip() for line in f_train_vi.readlines()]\n",
        "\n",
        "# Check whether length of en_train equal to length of vi_train\n",
        "assert len(en_train_lines) == len(vi_train_lines)\n",
        "\n",
        "train = pd.DataFrame({'en': en_train_lines, 'vi': vi_train_lines})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:23:04.803337Z",
          "iopub.execute_input": "2025-08-14T09:23:04.803619Z",
          "iopub.status.idle": "2025-08-14T09:23:07.960523Z",
          "shell.execute_reply.started": "2025-08-14T09:23:04.803603Z",
          "shell.execute_reply": "2025-08-14T09:23:07.959647Z"
        },
        "id": "EIP75gQAPH6P"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "en_test_file = '/content/drive/MyDrive/Released Corpus/test.en.txt'\n",
        "vi_test_file = '/content/drive/MyDrive/Released Corpus/test.vi.txt'\n",
        "\n",
        "# Read lines\n",
        "with open(en_test_file, 'r', encoding='utf-8') as f_test_en:\n",
        "    en_test_lines = [line.strip() for line in f_test_en.readlines()]\n",
        "\n",
        "with open(vi_test_file, 'r', encoding='utf-8') as f_test_vi:\n",
        "    vi_test_lines = [line.strip() for line in f_test_vi.readlines()]\n",
        "\n",
        "# Check whether length of en_train equal to length of vi_train\n",
        "assert len(en_test_lines) == len(vi_test_lines)\n",
        "\n",
        "test = pd.DataFrame({'en': en_test_lines, 'vi': vi_test_lines})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:23:07.961554Z",
          "iopub.execute_input": "2025-08-14T09:23:07.961796Z",
          "iopub.status.idle": "2025-08-14T09:23:07.996098Z",
          "shell.execute_reply.started": "2025-08-14T09:23:07.961779Z",
          "shell.execute_reply": "2025-08-14T09:23:07.995565Z"
        },
        "id": "vpF0ls2mPH6P"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Chia tập test thành val 20%, test 80%\n",
        "test, val = train_test_split(test, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:23:07.997395Z",
          "iopub.execute_input": "2025-08-14T09:23:07.997593Z",
          "iopub.status.idle": "2025-08-14T09:23:08.648302Z",
          "shell.execute_reply.started": "2025-08-14T09:23:07.997576Z",
          "shell.execute_reply": "2025-08-14T09:23:08.647742Z"
        },
        "id": "nXbydkXpPH6Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def english_preprocessing(text):\n",
        "    # Chuyển chữ hoa thành chữ thường\n",
        "    text = text.lower()\n",
        "\n",
        "    # Chuẩn hóa khoảng trắng ban đầu\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "    # Tách dấu hai chấm nếu dính liền (e.g., methods:This → methods: This)\n",
        "    text = re.sub(r'(?<=\\w):(?=\\w)', ': ', text)\n",
        "\n",
        "    # Xóa dấu ngoặc kép không cần thiết, nhưng giữ lại dấu nháy đơn trong từ (e.g., it's, don't)\n",
        "    text = re.sub(r'[“”\\\"`]', '', text)\n",
        "\n",
        "    # Xử lý đơn vị viết dính (e.g., 25ui/l → 25 ui/l)\n",
        "    text = re.sub(r'(\\d+)\\s*([a-zA-Z]+)', r'\\1 \\2', text)\n",
        "\n",
        "    # Chuẩn hóa số: \"81, 3%\" → \"81.3%\", \"9, 001\" → \"9001\"\n",
        "    text = re.sub(r'(\\d),\\s*(\\d)', r'\\1.\\2', text)  # 81, 3 → 81.3\n",
        "    text = re.sub(r'(?<=\\d)\\s*,\\s*(?=\\d)', '', text)  # 9, 001 → 9001\n",
        "\n",
        "    # Chuẩn hóa các loại dash\n",
        "    text = re.sub(r'[–—−]', '-', text)\n",
        "\n",
        "    # Giữ lại định dạng đúng cho các ký hiệu y học\n",
        "    text = re.sub(r'\\s*/\\s*', '/', text)   # PET / CT → PET/CT\n",
        "    text = re.sub(r'\\s*\\+\\s*', '+', text)  # ( + ) → (+)\n",
        "\n",
        "    # Thêm khoảng trắng quanh toán tử\n",
        "    text = re.sub(r'\\s*(<=|>=|=|≠|±|<|>)\\s*', r' \\1 ', text)\n",
        "\n",
        "    # Tách số và dấu %\n",
        "    text = re.sub(r'(\\d+(\\.\\d+)?)%', r'\\1 %', text)\n",
        "\n",
        "    # Làm sạch cuối cùng\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:23:08.648979Z",
          "iopub.execute_input": "2025-08-14T09:23:08.649254Z",
          "iopub.status.idle": "2025-08-14T09:23:08.655096Z",
          "shell.execute_reply.started": "2025-08-14T09:23:08.649238Z",
          "shell.execute_reply": "2025-08-14T09:23:08.654403Z"
        },
        "id": "FYLjZvOvPH6Q"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "def vietnamese_preprocessing(text):\n",
        "    # Chuyển chữ hoa thành chữ thường\n",
        "    text = text.lower()\n",
        "\n",
        "    # Chuẩn hóa khoảng trắng ban đầu\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "    # Tách dấu hai chấm nếu dính liền (e.g., methods:This → methods: This)\n",
        "    text = re.sub(r'(?<=\\w):(?=\\w)', ': ', text)\n",
        "\n",
        "    # Loại bỏ dấu câu không cần thiết\n",
        "    text = re.sub(r'[“”\\\"\\'`]', '', text)\n",
        "\n",
        "    # Xử lý đơn vị viết dính (e.g., 25ui/l → 25 ui/l)\n",
        "    text = re.sub(r'(\\d+)\\s*([a-zA-Z]+)', r'\\1 \\2', text)\n",
        "\n",
        "    # Chuẩn hóa số: \"81, 3%\" → \"81.3%\", \"9, 001\" → \"9001\"\n",
        "    text = re.sub(r'(\\d),\\s*(\\d)', r'\\1.\\2', text)  # 81, 3 → 81.3\n",
        "    text = re.sub(r'(?<=\\d)\\s*,\\s*(?=\\d)', '', text)  # 9, 001 → 9001\n",
        "\n",
        "    # Chuẩn hóa các loại dash\n",
        "    text = re.sub(r'[–—−]', '-', text)\n",
        "\n",
        "    # Giữ lại định dạng đúng cho các ký hiệu y học\n",
        "    text = re.sub(r'\\s*/\\s*', '/', text)   # PET / CT → PET/CT\n",
        "    text = re.sub(r'\\s*\\+\\s*', '+', text)  # ( + ) → (+)\n",
        "\n",
        "    # Thêm khoảng trắng quanh toán tử\n",
        "    text = re.sub(r'\\s*(<=|>=|=|≠|±|<|>)\\s*', r' \\1 ', text)\n",
        "\n",
        "    # Tách số và dấu %\n",
        "    text = re.sub(r'(\\d+(\\.\\d+)?)%', r'\\1 %', text)\n",
        "\n",
        "    # Làm sạch cuối cùng\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:23:08.655790Z",
          "iopub.execute_input": "2025-08-14T09:23:08.655955Z",
          "iopub.status.idle": "2025-08-14T09:23:08.671861Z",
          "shell.execute_reply.started": "2025-08-14T09:23:08.655941Z",
          "shell.execute_reply": "2025-08-14T09:23:08.671174Z"
        },
        "id": "c-cE74HDPH6Q"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "train['en'] = train['en'].apply(english_preprocessing)\n",
        "train['vi'] = train['vi'].apply(vietnamese_preprocessing)\n",
        "\n",
        "test['en'] = test['en'].apply(english_preprocessing)\n",
        "test['vi'] = test['vi'].apply(vietnamese_preprocessing)\n",
        "\n",
        "val['en'] = val['en'].apply(english_preprocessing)\n",
        "val['vi'] = val['vi'].apply(vietnamese_preprocessing)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:23:08.672565Z",
          "iopub.execute_input": "2025-08-14T09:23:08.672820Z",
          "iopub.status.idle": "2025-08-14T09:24:11.196441Z",
          "shell.execute_reply.started": "2025-08-14T09:23:08.672798Z",
          "shell.execute_reply": "2025-08-14T09:24:11.195911Z"
        },
        "id": "ZZ2VwVUWPH6R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "tokenizer_en = Tokenizer.from_file(\"/content/drive/MyDrive/Released Corpus/tokenizer_en.json\")\n",
        "tokenizer_vi = Tokenizer.from_file(\"/content/drive/MyDrive/Released Corpus/tokenizer_vi.json\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:24:11.197102Z",
          "iopub.execute_input": "2025-08-14T09:24:11.197313Z",
          "iopub.status.idle": "2025-08-14T09:24:11.440265Z",
          "shell.execute_reply.started": "2025-08-14T09:24:11.197295Z",
          "shell.execute_reply": "2025-08-14T09:24:11.439673Z"
        },
        "id": "-GjR41CGPH6R"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 256  # giới hạn chiều dài token sequence\n",
        "\n",
        "def encode_dataset(df, tokenizer_en, tokenizer_vi):\n",
        "    bos_en = tokenizer_en.token_to_id(\"<s>\")\n",
        "    eos_en = tokenizer_en.token_to_id(\"</s>\")\n",
        "    bos_vi = tokenizer_vi.token_to_id(\"<s>\")\n",
        "    eos_vi = tokenizer_vi.token_to_id(\"</s>\")\n",
        "\n",
        "    def truncate(seq, max_len):\n",
        "        return seq[:max_len] if len(seq) > max_len else seq\n",
        "\n",
        "    def encode_pair(en_text, vi_text):\n",
        "        # Encode English\n",
        "        en_ids = tokenizer_en.encode(en_text).ids\n",
        "        en_ids = truncate(en_ids, MAX_LEN - 2)  # trừ 2 vì thêm <s> và </s>\n",
        "        en_input = [bos_en] + en_ids + [eos_en]\n",
        "\n",
        "        # Encode Vietnamese (một lần)\n",
        "        vi_ids = tokenizer_vi.encode(vi_text).ids\n",
        "        vi_ids = truncate(vi_ids, MAX_LEN - 2)  # trừ 2 vì thêm <s> và </s>\n",
        "        vi_full = [bos_vi] + vi_ids + [eos_vi]\n",
        "\n",
        "        # Tách ra:\n",
        "        vi_input = vi_full[:-1]  # bỏ </s>\n",
        "        vi_target = vi_full[1:]  # bỏ <s>\n",
        "\n",
        "        return en_input, vi_input, vi_target\n",
        "\n",
        "    encoded = df.apply(lambda row: encode_pair(row['en'], row['vi']), axis=1)\n",
        "    en_input, vi_input, vi_target = zip(*encoded)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'en_input': en_input,\n",
        "        'vi_input': vi_input,\n",
        "        'vi_target': vi_target\n",
        "    })"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:24:11.440977Z",
          "iopub.execute_input": "2025-08-14T09:24:11.441163Z",
          "iopub.status.idle": "2025-08-14T09:24:11.447612Z",
          "shell.execute_reply.started": "2025-08-14T09:24:11.441149Z",
          "shell.execute_reply": "2025-08-14T09:24:11.446901Z"
        },
        "id": "zY7xRPKSPH6R"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoded = encode_dataset(train, tokenizer_en, tokenizer_vi)\n",
        "test_encoded = encode_dataset(test, tokenizer_en, tokenizer_vi)\n",
        "val_encoded = encode_dataset(val, tokenizer_en, tokenizer_vi)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:24:11.450073Z",
          "iopub.execute_input": "2025-08-14T09:24:11.450283Z",
          "iopub.status.idle": "2025-08-14T09:25:50.358012Z",
          "shell.execute_reply.started": "2025-08-14T09:24:11.450268Z",
          "shell.execute_reply": "2025-08-14T09:25:50.357177Z"
        },
        "id": "HF2oR1jkPH6R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "    super(InputEmbeddings, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "    # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.358797Z",
          "iopub.execute_input": "2025-08-14T09:25:50.359004Z",
          "iopub.status.idle": "2025-08-14T09:25:50.363494Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.358988Z",
          "shell.execute_reply": "2025-08-14T09:25:50.362779Z"
        },
        "id": "G2pwvtJqPH6S"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Create a matrix of shape (seq_len, d_model)\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "    # Create a vector of shape (seq_len)\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "    # Create a vector of shape (d_model)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
        "    # Apply sine to even indices\n",
        "    pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
        "    # Apply cosine to odd indices\n",
        "    pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
        "    # Add a batch dimension to the positional encoding\n",
        "    pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "    # Register the positional encoding as a buffer\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.364280Z",
          "iopub.execute_input": "2025-08-14T09:25:50.364475Z",
          "iopub.status.idle": "2025-08-14T09:25:50.378568Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.364459Z",
          "shell.execute_reply": "2025-08-14T09:25:50.377833Z"
        },
        "id": "au3l2lN0PH6S"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, h: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model # Embedding vector size\n",
        "    self.h = h # Number of heads\n",
        "\n",
        "    self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "      attention_scores = attention_scores.masked_fill(mask == 0, torch.finfo(attention_scores.dtype).min)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "    return torch.matmul(attention_scores, value), attention_scores\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "    # q, k, v: (batch, seq_len, d_model)\n",
        "    # mask: (batch, seq_len, seq_len)\n",
        "    query = self.w_q(q)\n",
        "    key = self.w_k(k)\n",
        "    value = self.w_v(v)\n",
        "\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "    # Calculate attention using function we will define next\n",
        "    x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    # Combine all the heads together\n",
        "    # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "    # Apply one final linear transformation\n",
        "    return self.w_o(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.379268Z",
          "iopub.execute_input": "2025-08-14T09:25:50.379772Z",
          "iopub.status.idle": "2025-08-14T09:25:50.393163Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.379755Z",
          "shell.execute_reply": "2025-08-14T09:25:50.392484Z"
        },
        "id": "2-8Zxn-KPH6S"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.393871Z",
          "iopub.execute_input": "2025-08-14T09:25:50.394122Z",
          "iopub.status.idle": "2025-08-14T09:25:50.407518Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.394093Z",
          "shell.execute_reply": "2025-08-14T09:25:50.406955Z"
        },
        "id": "M_NwlyqlPH6S"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, features: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = nn.LayerNorm(features)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    # Apply residual connection\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.408326Z",
          "iopub.execute_input": "2025-08-14T09:25:50.408901Z",
          "iopub.status.idle": "2025-08-14T09:25:50.420534Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.408882Z",
          "shell.execute_reply": "2025-08-14T09:25:50.419983Z"
        },
        "id": "63rMKluzPH6T"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "    return x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.421143Z",
          "iopub.execute_input": "2025-08-14T09:25:50.421357Z",
          "iopub.status.idle": "2025-08-14T09:25:50.435032Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.421341Z",
          "shell.execute_reply": "2025-08-14T09:25:50.434482Z"
        },
        "id": "gUEjM2dMPH6T"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = nn.LayerNorm(features)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.435826Z",
          "iopub.execute_input": "2025-08-14T09:25:50.436138Z",
          "iopub.status.idle": "2025-08-14T09:25:50.446466Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.436103Z",
          "shell.execute_reply": "2025-08-14T09:25:50.445789Z"
        },
        "id": "9e1FErGOPH6T"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "    x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "    x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "    return x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.447150Z",
          "iopub.execute_input": "2025-08-14T09:25:50.447376Z",
          "iopub.status.idle": "2025-08-14T09:25:50.454850Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.447360Z",
          "shell.execute_reply": "2025-08-14T09:25:50.454114Z"
        },
        "id": "TL7xhgFNPH6T"
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = nn.LayerNorm(features)\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.455561Z",
          "iopub.execute_input": "2025-08-14T09:25:50.455814Z",
          "iopub.status.idle": "2025-08-14T09:25:50.467328Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.455792Z",
          "shell.execute_reply": "2025-08-14T09:25:50.466680Z"
        },
        "id": "Z6V3jHIDPH6T"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "    return self.proj(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.468001Z",
          "iopub.execute_input": "2025-08-14T09:25:50.468222Z",
          "iopub.status.idle": "2025-08-14T09:25:50.479755Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.468183Z",
          "shell.execute_reply": "2025-08-14T09:25:50.479113Z"
        },
        "id": "zAxRHn9oPH6T"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.src_pos = src_pos\n",
        "    self.tgt_pos = tgt_pos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "    d_model = src_embed.d_model\n",
        "    self.src_pos_norm = nn.LayerNorm(d_model)\n",
        "    self.tgt_pos_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src = self.src_embed(src)\n",
        "    src = self.src_pos(src)\n",
        "    src = self.src_pos_norm(src)\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "  def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "    tgt = self.tgt_embed(tgt)\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    tgt = self.tgt_pos_norm(tgt)\n",
        "    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    return self.projection_layer(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:50.480427Z",
          "iopub.execute_input": "2025-08-14T09:25:50.480662Z",
          "iopub.status.idle": "2025-08-14T09:25:51.239470Z",
          "shell.execute_reply.started": "2025-08-14T09:25:50.480623Z",
          "shell.execute_reply": "2025-08-14T09:25:51.238700Z"
        },
        "id": "ZfewYT-bPH6T"
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderLayer(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderLayer(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "    # weight typing\n",
        "    projection_layer.proj.weight = tgt_embed.embedding.weight\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:51.240538Z",
          "iopub.execute_input": "2025-08-14T09:25:51.240784Z",
          "iopub.status.idle": "2025-08-14T09:25:51.256407Z",
          "shell.execute_reply.started": "2025-08-14T09:25:51.240759Z",
          "shell.execute_reply": "2025-08-14T09:25:51.255835Z"
        },
        "id": "4ScCDP6lPH6U"
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:51.257083Z",
          "iopub.execute_input": "2025-08-14T09:25:51.257279Z",
          "iopub.status.idle": "2025-08-14T09:25:51.269410Z",
          "shell.execute_reply.started": "2025-08-14T09:25:51.257264Z",
          "shell.execute_reply": "2025-08-14T09:25:51.268841Z"
        },
        "id": "0GDzNZb4PH6U"
      },
      "outputs": [],
      "execution_count": 49
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.src = df['en_input']\n",
        "        self.tgt_in = df['vi_input']\n",
        "        self.tgt_out = df['vi_target']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Chỉ trả về tensor, không padding\n",
        "        return {\n",
        "            'src': torch.tensor(self.src.iloc[idx], dtype=torch.long),\n",
        "            'tgt_in': torch.tensor(self.tgt_in.iloc[idx], dtype=torch.long),\n",
        "            'tgt_out': torch.tensor(self.tgt_out.iloc[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Hàm collate_fn\n",
        "def collate_fn(batch):\n",
        "    pad_id = tokenizer_vi.token_to_id(\"<pad>\")\n",
        "\n",
        "    src_batch = [item['src'] for item in batch]\n",
        "    tgt_in_batch = [item['tgt_in'] for item in batch]\n",
        "    tgt_out_batch = [item['tgt_out'] for item in batch]\n",
        "\n",
        "    # pad_sequence sẽ tự động pad đến độ dài lớn nhất trong batch\n",
        "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=pad_id)\n",
        "    tgt_in_padded = pad_sequence(tgt_in_batch, batch_first=True, padding_value=pad_id)\n",
        "    tgt_out_padded = pad_sequence(tgt_out_batch, batch_first=True, padding_value=pad_id)\n",
        "\n",
        "    return {\n",
        "        'src': src_padded,\n",
        "        'tgt_in': tgt_in_padded,\n",
        "        'tgt_out': tgt_out_padded\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:51.270163Z",
          "iopub.execute_input": "2025-08-14T09:25:51.270409Z",
          "iopub.status.idle": "2025-08-14T09:25:51.283144Z",
          "shell.execute_reply.started": "2025-08-14T09:25:51.270385Z",
          "shell.execute_reply": "2025-08-14T09:25:51.282448Z"
        },
        "id": "3w8PSd6KPH6U"
      },
      "outputs": [],
      "execution_count": 50
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TranslationDataset(train_encoded)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "test_dataset = TranslationDataset(test_encoded)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "val_dataset = TranslationDataset(val_encoded)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:51.283816Z",
          "iopub.execute_input": "2025-08-14T09:25:51.284024Z",
          "iopub.status.idle": "2025-08-14T09:25:51.297850Z",
          "shell.execute_reply.started": "2025-08-14T09:25:51.284004Z",
          "shell.execute_reply": "2025-08-14T09:25:51.297076Z"
        },
        "id": "WvZvctelPH6U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm tạo mask\n",
        "def create_mask(src, tgt, pad_id=tokenizer_en.token_to_id(\"<pad>\")):\n",
        "    # src: (batch, src_len), tgt: (batch, tgt_len)\n",
        "    src_mask = (src != pad_id).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, src_len)\n",
        "    tgt_pad_mask = (tgt != pad_id).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, tgt_len)\n",
        "    tgt_len = tgt.size(1)\n",
        "    tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()  # (tgt_len, tgt_len)\n",
        "    tgt_mask = tgt_pad_mask & tgt_sub_mask  # (batch, 1, tgt_len, tgt_len)\n",
        "    return src_mask, tgt_mask"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:51.298552Z",
          "iopub.execute_input": "2025-08-14T09:25:51.298861Z",
          "iopub.status.idle": "2025-08-14T09:25:51.310216Z",
          "shell.execute_reply.started": "2025-08-14T09:25:51.298837Z",
          "shell.execute_reply": "2025-08-14T09:25:51.309521Z"
        },
        "id": "mmkOJx8nPH6U"
      },
      "outputs": [],
      "execution_count": 51
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, loss_fn, device, scheduler=None):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        src = batch['src'].to(device)\n",
        "        tgt_input = batch['tgt_in'].to(device)\n",
        "        tgt_output = batch['tgt_out'].to(device)\n",
        "\n",
        "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(device_type=device.type):  # mixed precision context\n",
        "            encoder_output = model.encode(src, src_mask)\n",
        "            decoder_output = model.decode(encoder_output, src_mask, tgt_input, tgt_mask)\n",
        "            output = model.project(decoder_output)\n",
        "\n",
        "            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
        "\n",
        "        # backward and step using scaler\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:51.310873Z",
          "iopub.execute_input": "2025-08-14T09:25:51.311087Z",
          "iopub.status.idle": "2025-08-14T09:25:51.406477Z",
          "shell.execute_reply.started": "2025-08-14T09:25:51.311053Z",
          "shell.execute_reply": "2025-08-14T09:25:51.405905Z"
        },
        "id": "fECJxSdTPH6V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_one_epoch(model, dataloader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        src = batch['src'].to(device)\n",
        "        tgt_input = batch['tgt_in'].to(device)\n",
        "        tgt_output = batch['tgt_out'].to(device)\n",
        "\n",
        "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        encoder_output = model.encode(src, src_mask)\n",
        "        decoder_output = model.decode(encoder_output, src_mask, tgt_input, tgt_mask)\n",
        "        output = model.project(decoder_output)\n",
        "\n",
        "        loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(val_loss=loss.item())\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:51.409792Z",
          "iopub.execute_input": "2025-08-14T09:25:51.409994Z",
          "iopub.status.idle": "2025-08-14T09:25:51.418556Z",
          "shell.execute_reply.started": "2025-08-14T09:25:51.409979Z",
          "shell.execute_reply": "2025-08-14T09:25:51.418013Z"
        },
        "id": "GU3kobySPH6W"
      },
      "outputs": [],
      "execution_count": 52
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupInverseSquareRootScheduler:\n",
        "    def __init__(self, optimizer, d_model, warmup_steps):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.d_model = d_model\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        lr = self._get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    def _get_lr(self):\n",
        "        arg1 = self.step_num ** (-0.5)\n",
        "        arg2 = self.step_num * (self.warmup_steps ** (-1.5))\n",
        "        return (self.d_model ** -0.5) * min(arg1, arg2)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:51.419266Z",
          "iopub.execute_input": "2025-08-14T09:25:51.419506Z",
          "iopub.status.idle": "2025-08-14T09:25:51.432435Z",
          "shell.execute_reply.started": "2025-08-14T09:25:51.419485Z",
          "shell.execute_reply": "2025-08-14T09:25:51.431927Z"
        },
        "id": "n3XNdaNePH6W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "src_vocab_size = tokenizer_en.get_vocab_size()\n",
        "tgt_vocab_size = tokenizer_vi.get_vocab_size()\n",
        "\n",
        "model = build_transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    src_seq_len=256,\n",
        "    tgt_seq_len=256,\n",
        "    d_model=512,\n",
        "    N=6,\n",
        "    h=8,\n",
        "    dropout=0.1,\n",
        "    d_ff=1024\n",
        ").to(device)\n",
        "\n",
        "# Loss with label smoothing\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_vi.token_to_id(\"<pad>\"), label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Tạo Scheduler\n",
        "scheduler = WarmupInverseSquareRootScheduler(optimizer, d_model=512, warmup_steps=4000)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:51.433163Z",
          "iopub.execute_input": "2025-08-14T09:25:51.433405Z",
          "iopub.status.idle": "2025-08-14T09:25:55.391798Z",
          "shell.execute_reply.started": "2025-08-14T09:25:51.433379Z",
          "shell.execute_reply": "2025-08-14T09:25:55.391238Z"
        },
        "id": "XpXe9t1_PH6X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(size):\n",
        "    \"\"\"\n",
        "    Tạo causal mask: chỉ cho phép mỗi vị trí nhìn thấy các token trước đó (kể cả chính nó).\n",
        "    Output shape: (1, 1, size, size)\n",
        "    \"\"\"\n",
        "    return torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(1)  # (1, 1, size, size)\n",
        "\n",
        "def beam_search_decode_batch_parallel(\n",
        "    model, src, src_mask, tokenizer_vi, max_len, device, beam_size=4, alpha=0.6\n",
        "):\n",
        "    def get_token_id(tok, fallback=None):\n",
        "        tid = tokenizer_vi.token_to_id(tok)\n",
        "        if tid is None and fallback is not None:\n",
        "            tid = tokenizer_vi.token_to_id(fallback)\n",
        "        return tid\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        sos_id = get_token_id(\"<s>\", \"[SOS]\")\n",
        "        eos_id = get_token_id(\"</s>\", \"[EOS]\")\n",
        "        pad_id = get_token_id(\"<pad>\", \"[PAD]\")\n",
        "\n",
        "        if sos_id is None or eos_id is None or pad_id is None:\n",
        "            raise ValueError(f\"Special tokens missing: sos={sos_id}, eos={eos_id}, pad={pad_id}\")\n",
        "\n",
        "        def length_penalty_fn(length, alpha):\n",
        "            return ((5.0 + length) ** alpha) / ((5.0 + 1.0) ** alpha)\n",
        "\n",
        "        # Encode input\n",
        "        memory = model.encode(src, src_mask)\n",
        "        memory = memory.repeat_interleave(beam_size, dim=0)\n",
        "        src_mask = src_mask.repeat_interleave(beam_size, dim=0)\n",
        "\n",
        "        seqs = torch.full((batch_size * beam_size, 1), sos_id, dtype=torch.long, device=device)\n",
        "        scores = torch.zeros(batch_size * beam_size, device=device)\n",
        "        finished_flags = torch.zeros(batch_size * beam_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        for step in range(1, max_len):\n",
        "            tgt_mask = causal_mask(seqs.size(1)).to(device)\n",
        "            dec_out = model.decode(memory, src_mask, seqs, tgt_mask)\n",
        "            logits = model.project(dec_out[:, -1, :])\n",
        "            log_probs = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "            # Beam đã kết thúc chỉ sinh <pad>\n",
        "            log_probs[finished_flags] = -1e9\n",
        "            log_probs[finished_flags, pad_id] = 0\n",
        "\n",
        "            next_scores, next_tokens = torch.topk(log_probs, beam_size, dim=-1)\n",
        "\n",
        "            # Reshape để chọn top beam cho batch\n",
        "            next_scores = next_scores.view(batch_size, beam_size, beam_size)\n",
        "            next_tokens = next_tokens.view(batch_size, beam_size, beam_size)\n",
        "\n",
        "            # Tính total score với length penalty sớm\n",
        "            total_scores = scores.view(batch_size, beam_size, 1) + next_scores\n",
        "            lp = length_penalty_fn(step + 1, alpha)\n",
        "            total_scores = total_scores / lp\n",
        "\n",
        "            # Chọn top beam\n",
        "            top_scores, top_indices = torch.topk(total_scores.view(batch_size, -1), beam_size, dim=-1)\n",
        "            beam_indices = top_indices // beam_size\n",
        "            token_indices = top_indices % beam_size\n",
        "\n",
        "            # Vector hóa update sequences\n",
        "            old_beam_ids = (beam_indices + torch.arange(batch_size, device=device).unsqueeze(1) * beam_size).view(-1)\n",
        "            chosen_tokens = next_tokens[torch.arange(batch_size, device=device).unsqueeze(1), beam_indices, token_indices].view(-1)\n",
        "\n",
        "            seqs = torch.cat([seqs[old_beam_ids], chosen_tokens.unsqueeze(1)], dim=-1)\n",
        "            scores = top_scores.view(-1)\n",
        "\n",
        "            # Cập nhật finished flags\n",
        "            finished_flags = finished_flags[old_beam_ids] | (chosen_tokens == eos_id)\n",
        "\n",
        "            if finished_flags.all():\n",
        "                break\n",
        "\n",
        "        # Chọn best beam cuối cùng\n",
        "        final_seqs = []\n",
        "        for b in range(batch_size):\n",
        "            start = b * beam_size\n",
        "            end = start + beam_size\n",
        "            cand_scores = scores[start:end]\n",
        "            cand_seqs = seqs[start:end]\n",
        "            lengths = torch.tensor([len(s) for s in cand_seqs], dtype=torch.float, device=device)\n",
        "            lp = length_penalty_fn(lengths, alpha)\n",
        "            best_idx = torch.argmax(cand_scores / lp).item()\n",
        "            final_seqs.append(cand_seqs[best_idx])\n",
        "\n",
        "        # Pad output\n",
        "        max_len_final = max(len(s) for s in final_seqs)\n",
        "        padded = torch.full((batch_size, max_len_final), pad_id, dtype=torch.long, device=device)\n",
        "        for i, seq in enumerate(final_seqs):\n",
        "            padded[i, :len(seq)] = seq\n",
        "\n",
        "        return padded"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T15:31:56.636878Z",
          "iopub.execute_input": "2025-08-14T15:31:56.637530Z",
          "iopub.status.idle": "2025-08-14T15:31:56.649873Z",
          "shell.execute_reply.started": "2025-08-14T15:31:56.637507Z",
          "shell.execute_reply": "2025-08-14T15:31:56.649148Z"
        },
        "id": "kxDfDcetPH6X"
      },
      "outputs": [],
      "execution_count": 53
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "def evaluate_bleu(model, dataloader, tokenizer_en, tokenizer_vi, device, max_len=128):\n",
        "    model.eval()\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    sos_id = tokenizer_vi.token_to_id(\"[SOS]\") or tokenizer_vi.token_to_id(\"<s>\")\n",
        "    eos_id = tokenizer_vi.token_to_id(\"[EOS]\") or tokenizer_vi.token_to_id(\"</s>\")\n",
        "    pad_id = tokenizer_vi.token_to_id(\"<pad>\")\n",
        "    pad_src_id = tokenizer_en.token_to_id(\"<pad>\")\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating BLEU\"):\n",
        "        src = batch['src'].to(device)\n",
        "        tgt_input = batch['tgt_in'].to(device)\n",
        "        tgt_output = batch['tgt_out'].to(device)\n",
        "\n",
        "        src_mask = (src != pad_src_id).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        pred_ids_batch = beam_search_decode_batch_parallel(model, src, src_mask, tokenizer_vi, max_len, device, beam_size=4)\n",
        "\n",
        "        for pred_ids, ref_ids in zip(pred_ids_batch, tgt_output):\n",
        "            pred_tokens = [\n",
        "                tokenizer_vi.id_to_token(id.item())\n",
        "                for id in pred_ids\n",
        "                if id.item() not in {sos_id, eos_id, pad_id}\n",
        "            ]\n",
        "\n",
        "            ref_tokens = [\n",
        "                tokenizer_vi.id_to_token(id.item())\n",
        "                for id in ref_ids\n",
        "                if id.item() not in {sos_id, eos_id, pad_id}\n",
        "            ]\n",
        "\n",
        "            hypotheses.append(pred_tokens)\n",
        "            references.append([ref_tokens])  # each reference must be a list of lists\n",
        "\n",
        "    bleu = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
        "\n",
        "    print(f\"BLEU score (nltk): {bleu * 100:.2f}\")\n",
        "    return bleu"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T15:32:00.222676Z",
          "iopub.execute_input": "2025-08-14T15:32:00.222947Z",
          "iopub.status.idle": "2025-08-14T15:32:00.230270Z",
          "shell.execute_reply.started": "2025-08-14T15:32:00.222927Z",
          "shell.execute_reply": "2025-08-14T15:32:00.229556Z"
        },
        "id": "ipwwhH4tPH6X"
      },
      "outputs": [],
      "execution_count": 54
    },
    {
      "cell_type": "code",
      "source": [
        "# Load lại model\n",
        "model = build_transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    src_seq_len=256,\n",
        "    tgt_seq_len=256,\n",
        "    d_model=512,\n",
        "    N=6,\n",
        "    h=8,\n",
        "    dropout=0.1,\n",
        "    d_ff=1024\n",
        ").to(device)\n",
        "\n",
        "checkpoint_path = '/kaggle/input/model-pretrained/transformer_best.pth'\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)  # Tạo lại optimizer tương ứng\n",
        "scaler = GradScaler()  # ✅ Đặt ở đây sau khi tạo model + optimizer\n",
        "\n",
        "scheduler = WarmupInverseSquareRootScheduler(optimizer, d_model=512, warmup_steps=4000)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T15:32:06.996282Z",
          "iopub.execute_input": "2025-08-14T15:32:06.996838Z",
          "iopub.status.idle": "2025-08-14T15:32:08.487170Z",
          "shell.execute_reply.started": "2025-08-14T15:32:06.996813Z",
          "shell.execute_reply": "2025-08-14T15:32:08.486594Z"
        },
        "id": "acoYm85aPH6Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = 2.1\n",
        "early_stop_counter = 0\n",
        "patience = 5\n",
        "\n",
        "for epoch in range(15):\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    train_loss = train_one_epoch(model, train_dataloader, optimizer, loss_fn, device, scheduler=scheduler)\n",
        "    val_loss = evaluate_one_epoch(model, val_dataloader, loss_fn, device)\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    # Kiểm tra cải thiện\n",
        "    improved = False\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        improved = True\n",
        "\n",
        "    if improved:\n",
        "        early_stop_counter = 0\n",
        "        save_path = '/kaggle/working/transformer_best.pth'\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(\"Saved model!\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"No improvement after several epochs. Early stopping.\")\n",
        "            break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:25:59.264844Z",
          "iopub.execute_input": "2025-08-14T09:25:59.265112Z",
          "iopub.status.idle": "2025-08-14T15:13:13.653966Z",
          "shell.execute_reply.started": "2025-08-14T09:25:59.265088Z",
          "shell.execute_reply": "2025-08-14T15:13:13.652686Z"
        },
        "id": "GntNlwoAPH6Y",
        "outputId": "bead13e0-d75c-4e1c-c14c-1d2423f11b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 1.9972 | Val Loss: 2.1382\nEpoch 2\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 1.9127 | Val Loss: 2.1084\nEpoch 3\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 1.8828 | Val Loss: 2.1023\nEpoch 4\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 1.8627 | Val Loss: 2.0786\nSaved model!\nEpoch 5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 1.8350 | Val Loss: 2.0806\nEpoch 6\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 1.8240 | Val Loss: 2.0791\nEpoch 7\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 1.8248 | Val Loss: 2.0695\nSaved model!\nEpoch 8\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                           \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train Loss: 1.8102 | Val Loss: 2.0712\nEpoch 9\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                          \r",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_36/576880503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_36/3328836064.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, device, scheduler)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "src_vocab_size = tokenizer_en.get_vocab_size()\n",
        "tgt_vocab_size = tokenizer_vi.get_vocab_size()\n",
        "\n",
        "# Load lại model\n",
        "model = build_transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    src_seq_len=256,\n",
        "    tgt_seq_len=256,\n",
        "    d_model=512,\n",
        "    N=6,\n",
        "    h=8,\n",
        "    dropout=0.1,\n",
        "    d_ff=1024\n",
        ").to(device)\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/Released Corpus/transformer_best(47).pth'\n",
        "model.load_state_dict(torch.load(checkpoint_path))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T15:32:26.604942Z",
          "iopub.execute_input": "2025-08-14T15:32:26.605500Z",
          "iopub.status.idle": "2025-08-14T15:32:28.015870Z",
          "shell.execute_reply.started": "2025-08-14T15:32:26.605478Z",
          "shell.execute_reply": "2025-08-14T15:32:28.015120Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FDP-Ye9PH6Z",
        "outputId": "cd3e4fd1-6c7b-426f-e493-c3449fb0c035"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": [
        "en_test_file = '/content/drive/MyDrive/Released Corpus/test.en.txt'\n",
        "vi_test_file = '/content/drive/MyDrive/Released Corpus/test.vi.txt'\n",
        "\n",
        "# Read lines\n",
        "with open(en_test_file, 'r', encoding='utf-8') as f_test_en:\n",
        "    en_test_lines = [line.strip() for line in f_test_en.readlines()]\n",
        "\n",
        "with open(vi_test_file, 'r', encoding='utf-8') as f_test_vi:\n",
        "    vi_test_lines = [line.strip() for line in f_test_vi.readlines()]\n",
        "\n",
        "# Check whether length of en_train equal to length of vi_train\n",
        "assert len(en_test_lines) == len(vi_test_lines)\n",
        "\n",
        "test = pd.DataFrame({'en': en_test_lines, 'vi': vi_test_lines})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T15:32:37.589662Z",
          "iopub.execute_input": "2025-08-14T15:32:37.590219Z",
          "iopub.status.idle": "2025-08-14T15:32:37.611338Z",
          "shell.execute_reply.started": "2025-08-14T15:32:37.590195Z",
          "shell.execute_reply": "2025-08-14T15:32:37.610802Z"
        },
        "id": "J1gPBMI4PH6Z"
      },
      "outputs": [],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T15:32:42.107311Z",
          "iopub.execute_input": "2025-08-14T15:32:42.107547Z",
          "iopub.status.idle": "2025-08-14T15:32:42.125417Z",
          "shell.execute_reply.started": "2025-08-14T15:32:42.107531Z",
          "shell.execute_reply": "2025-08-14T15:32:42.124878Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "AopXiqb6PH6Z",
        "outputId": "e9da3ff7-1a1d-476b-8cb0-b78eeb928abd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     en  \\\n",
              "0     Knowledge, practices in public health service ...   \n",
              "1     Describe knowledge, practices in public health...   \n",
              "2     Methodology: A cross sectional study was used ...   \n",
              "3     Results: Percentage of card's holders who knew...   \n",
              "4     Percentage of card's holders who went to the f...   \n",
              "...                                                 ...   \n",
              "2995  Therefore, we conduct research to evaluate the...   \n",
              "2996  Methods: A cross-sectional descriptive study w...   \n",
              "2997  Results: In 169 patients, 23.1% and 19.5% pati...   \n",
              "2998  Self-tanning products do not provide significa...   \n",
              "2999  Benzathine penicillin G is a long-acting formu...   \n",
              "\n",
              "                                                     vi  \n",
              "0     Thực trạng kiến thức và thực hành của người có...  \n",
              "1     Mô tả thực trạng kiến thức, thực hành của ngườ...  \n",
              "2     Phương pháp: Thiết kế nghiên mô tả cắt ngang đ...  \n",
              "3     Kết quả: Tỷ lệ người biết được khám chữa bệnh ...  \n",
              "4     Tỷ lệ người có thẻ BHYT thực hành khám chữa bệ...  \n",
              "...                                                 ...  \n",
              "2995  Chính vì vậy chúng tôi tiến hành nghiên cứu đá...  \n",
              "2996  Phương pháp nghiên cứu: Nghiên cứu mô tả cắt n...  \n",
              "2997  Số lượng bệnh nhân bị di căn hạch rốn phổi cao...  \n",
              "2998  Các sản phẩm tự tạo màu da không có khả năng b...  \n",
              "2999  Benzathine penicillin G là một dạng thuốc có t...  \n",
              "\n",
              "[3000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-84d4f4f5-2b30-4e78-9cb3-09680526123d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>vi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Knowledge, practices in public health service ...</td>\n",
              "      <td>Thực trạng kiến thức và thực hành của người có...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Describe knowledge, practices in public health...</td>\n",
              "      <td>Mô tả thực trạng kiến thức, thực hành của ngườ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Methodology: A cross sectional study was used ...</td>\n",
              "      <td>Phương pháp: Thiết kế nghiên mô tả cắt ngang đ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Results: Percentage of card's holders who knew...</td>\n",
              "      <td>Kết quả: Tỷ lệ người biết được khám chữa bệnh ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Percentage of card's holders who went to the f...</td>\n",
              "      <td>Tỷ lệ người có thẻ BHYT thực hành khám chữa bệ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>Therefore, we conduct research to evaluate the...</td>\n",
              "      <td>Chính vì vậy chúng tôi tiến hành nghiên cứu đá...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>Methods: A cross-sectional descriptive study w...</td>\n",
              "      <td>Phương pháp nghiên cứu: Nghiên cứu mô tả cắt n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>Results: In 169 patients, 23.1% and 19.5% pati...</td>\n",
              "      <td>Số lượng bệnh nhân bị di căn hạch rốn phổi cao...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>Self-tanning products do not provide significa...</td>\n",
              "      <td>Các sản phẩm tự tạo màu da không có khả năng b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>Benzathine penicillin G is a long-acting formu...</td>\n",
              "      <td>Benzathine penicillin G là một dạng thuốc có t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84d4f4f5-2b30-4e78-9cb3-09680526123d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-84d4f4f5-2b30-4e78-9cb3-09680526123d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-84d4f4f5-2b30-4e78-9cb3-09680526123d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4175cde8-b324-4eb6-afba-7114d8f0db14\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4175cde8-b324-4eb6-afba-7114d8f0db14')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4175cde8-b324-4eb6-afba-7114d8f0db14 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_5e625a57-e8a8-4286-84cb-8470acadb09f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5e625a57-e8a8-4286-84cb-8470acadb09f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test",
              "summary": "{\n  \"name\": \"test\",\n  \"rows\": 3000,\n  \"fields\": [\n    {\n      \"column\": \"en\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2997,\n        \"samples\": [\n          \"Objective: To determine the proportion of anxiety, depression and the association with the duration of taking oral isotretinoin in patients with acne at Hospital of Dermato-Venereology, Ho Chi Minh city.\",\n          \"Subjects and methods: Clinical research, randomized controlled trials (RCTs) of 120 patients with post-herpetic neuralgia at National Hospital of Dermatology and Venereology from July 2020 to May 2021.Results: Mean age of research and control group were 66.43 \\u00b1 8.67 and 66, 57 \\u00b1 8, 66, repecstively.\",\n          \"Conclusion: Our study has shown that the percutaneous closure of large secondary atrial septal defects in the 20 - 37 mm diameter range under intracardiac echocardiography guidance can be performed safely and effectively.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vi\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2995,\n        \"samples\": [\n          \"T\\u1eeb kho\\u00e1: C\\u1ea3i ti\\u1ebfn, hi\\u1ec3u bi\\u1ebft, ng\\u01b0\\u1eddi b\\u1ec7nh v\\u00e0 gia \\u0111\\u00ecnh, ch\\u0103m s\\u00f3c.\",\n          \"Vi\\u1ec7c n\\u00e0y c\\u00f3 th\\u1ec3 \\u0111\\u01b0\\u1ee3c th\\u1ef1c hi\\u1ec7n t\\u1ea1i ph\\u00f2ng c\\u1ea5p c\\u1ee9u ho\\u1eb7c ph\\u00f2ng kh\\u00e1m ngo\\u1ea1i tr\\u00fa.\",\n          \"\\u0110\\u00e1nh gi\\u00e1 c\\u00e1c y\\u1ebfu t\\u1ed1 ti\\u00ean l\\u01b0\\u1ee3ng th\\u00e0nh c\\u00f4ng c\\u1ee7a k\\u1ef9 thu\\u1eadt th\\u1edf oxy l\\u00e0m \\u1ea9m d\\u00f2ng cao qua canuyn m\\u0169i (HHFNC) tr\\u00ean b\\u1ec7nh nh\\u00e2n \\u0111\\u1ee3t c\\u1ea5p b\\u1ec7nh ph\\u1ed5i t\\u1eafc ngh\\u1ebdn m\\u1ea1n t\\u00ednh (COPD).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "execution_count": 41
    },
    {
      "cell_type": "code",
      "source": [
        "test['en'] = test['en'].apply(english_preprocessing)\n",
        "test['vi'] = test['vi'].apply(vietnamese_preprocessing)\n",
        "\n",
        "test_encoded = encode_dataset(test, tokenizer_en, tokenizer_vi)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T15:32:45.093533Z",
          "iopub.execute_input": "2025-08-14T15:32:45.094232Z",
          "iopub.status.idle": "2025-08-14T15:32:46.005762Z",
          "shell.execute_reply.started": "2025-08-14T15:32:45.094208Z",
          "shell.execute_reply": "2025-08-14T15:32:46.005206Z"
        },
        "id": "Lm6wsSy_PH6a"
      },
      "outputs": [],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TranslationDataset(test_encoded)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T15:32:53.124284Z",
          "iopub.execute_input": "2025-08-14T15:32:53.124951Z",
          "iopub.status.idle": "2025-08-14T15:32:53.128557Z",
          "shell.execute_reply.started": "2025-08-14T15:32:53.124930Z",
          "shell.execute_reply": "2025-08-14T15:32:53.127822Z"
        },
        "id": "09xkBdA1PH6a"
      },
      "outputs": [],
      "execution_count": 43
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bleu(model, test_dataloader, tokenizer_en, tokenizer_vi, device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T15:32:56.126474Z",
          "iopub.execute_input": "2025-08-14T15:32:56.126958Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NTHeaAjPH6e",
        "outputId": "025a9804-63bb-4357-f7bb-e49e4a129ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating BLEU: 100%|██████████| 94/94 [15:59<00:00, 10.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score (nltk): 57.44\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5743942868709628"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "execution_count": 55
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_vietnamese(sentence_en: str) -> str:\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Tiền xử lý tiếng Anh\n",
        "    src_text = english_preprocessing(sentence_en)\n",
        "    src_ids = tokenizer_en.encode(src_text).ids\n",
        "    src_ids = [tokenizer_en.token_to_id(\"<s>\")] + src_ids + [tokenizer_en.token_to_id(\"</s>\")]\n",
        "\n",
        "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    pad_id = tokenizer_en.token_to_id(\"<pad>\")\n",
        "    assert pad_id is not None, \"Tokenizer English chưa có token <pad>\"\n",
        "    src_mask = (src_tensor != pad_id).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_ids = beam_search_decode_batch_parallel(\n",
        "            model, src_tensor, src_mask, tokenizer_vi,\n",
        "            max_len=128, device=device, beam_size=4, alpha=0.6\n",
        "        )[0]\n",
        "\n",
        "    pred_ids = pred_ids.cpu().numpy()\n",
        "\n",
        "    # Dùng decode của tokenizer để ghép subword đúng chuẩn\n",
        "    return tokenizer_vi.decode(pred_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T15:13:13.660596Z",
          "iopub.status.idle": "2025-08-14T15:13:13.660868Z",
          "shell.execute_reply.started": "2025-08-14T15:13:13.660754Z",
          "shell.execute_reply": "2025-08-14T15:13:13.660764Z"
        },
        "id": "QbL-O3tbPH6e"
      },
      "outputs": [],
      "execution_count": 56
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "vi_output = []\n",
        "\n",
        "# Dịch toàn bộ 3000 câu tiếng Anh\n",
        "for sentence in tqdm.tqdm(test['en'], desc=\"Translating test set\"):\n",
        "    translation = translate_to_vietnamese(sentence)\n",
        "    vi_output.append(translation)\n",
        "\n",
        "# Tạo dataframe mới\n",
        "df_result = pd.DataFrame({\n",
        "    'en': test['en'],          # câu tiếng Anh gốc\n",
        "    'vi_output': vi_output,    # câu dịch từ model\n",
        "    'vi_truth': test['vi']     # câu ground truth\n",
        "})\n",
        "\n",
        "# Lưu vào CSV\n",
        "df_result.to_csv(\"Transformer-test-translation.csv\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYkG1CO2PH6f",
        "outputId": "55c3df31-870d-40e1-a0c5-3810bbf4a584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating test set: 100%|██████████| 3000/3000 [16:07<00:00,  3.10it/s]\n"
          ]
        }
      ],
      "execution_count": 57
    }
  ]
}