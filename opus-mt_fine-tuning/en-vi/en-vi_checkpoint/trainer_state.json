{
  "best_global_step": 56250,
  "best_metric": 2.2573838233947754,
  "best_model_checkpoint": "opus-mt-en-vi-medical-finetuned/checkpoint-56250",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 56250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 1.497651219367981,
      "learning_rate": 0.00029822577777777773,
      "loss": 3.2783,
      "step": 500
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 1.4335709810256958,
      "learning_rate": 0.000296448,
      "loss": 3.0533,
      "step": 1000
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 1.534197211265564,
      "learning_rate": 0.0002946702222222222,
      "loss": 2.9894,
      "step": 1500
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 1.5683414936065674,
      "learning_rate": 0.0002928924444444444,
      "loss": 2.9418,
      "step": 2000
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 1.4219075441360474,
      "learning_rate": 0.0002911146666666667,
      "loss": 2.8903,
      "step": 2500
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 1.5313860177993774,
      "learning_rate": 0.00028933688888888883,
      "loss": 2.8608,
      "step": 3000
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 1.7534570693969727,
      "learning_rate": 0.0002875591111111111,
      "loss": 2.8355,
      "step": 3500
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 1.6492464542388916,
      "learning_rate": 0.0002857813333333333,
      "loss": 2.8152,
      "step": 4000
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.7760628461837769,
      "learning_rate": 0.0002840035555555555,
      "loss": 2.7564,
      "step": 4500
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.3510710000991821,
      "learning_rate": 0.0002822257777777778,
      "loss": 2.7722,
      "step": 5000
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 1.680011510848999,
      "learning_rate": 0.000280448,
      "loss": 2.7457,
      "step": 5500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 1.4725476503372192,
      "learning_rate": 0.0002786702222222222,
      "loss": 2.7408,
      "step": 6000
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 1.5131033658981323,
      "learning_rate": 0.0002768924444444444,
      "loss": 2.7384,
      "step": 6500
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 1.4096910953521729,
      "learning_rate": 0.00027511466666666666,
      "loss": 2.7219,
      "step": 7000
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.6642498970031738,
      "learning_rate": 0.00027333688888888887,
      "loss": 2.6998,
      "step": 7500
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 1.4682815074920654,
      "learning_rate": 0.0002715591111111111,
      "loss": 2.6973,
      "step": 8000
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 1.486690640449524,
      "learning_rate": 0.0002697813333333333,
      "loss": 2.6809,
      "step": 8500
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.506880521774292,
      "learning_rate": 0.0002680035555555555,
      "loss": 2.7139,
      "step": 9000
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 1.4962983131408691,
      "learning_rate": 0.00026622577777777776,
      "loss": 2.6725,
      "step": 9500
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 1.9313817024230957,
      "learning_rate": 0.00026444799999999997,
      "loss": 2.6496,
      "step": 10000
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.422704815864563,
      "learning_rate": 0.00026267022222222224,
      "loss": 2.6785,
      "step": 10500
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 1.616334319114685,
      "learning_rate": 0.0002608924444444444,
      "loss": 2.6533,
      "step": 11000
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 1.5939090251922607,
      "learning_rate": 0.00025911466666666665,
      "loss": 2.6675,
      "step": 11500
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 1.2953355312347412,
      "learning_rate": 0.00025733688888888886,
      "loss": 2.627,
      "step": 12000
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.529039740562439,
      "learning_rate": 0.00025555911111111107,
      "loss": 2.6375,
      "step": 12500
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 1.6250598430633545,
      "learning_rate": 0.00025378133333333334,
      "loss": 2.6315,
      "step": 13000
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.6076825857162476,
      "learning_rate": 0.00025200355555555554,
      "loss": 2.6456,
      "step": 13500
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 1.4250408411026,
      "learning_rate": 0.00025022577777777775,
      "loss": 2.6504,
      "step": 14000
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 1.401054859161377,
      "learning_rate": 0.00024844799999999996,
      "loss": 2.6233,
      "step": 14500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.056704521179199,
      "learning_rate": 0.0002466702222222222,
      "loss": 2.6356,
      "step": 15000
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 1.8380993604660034,
      "learning_rate": 0.00024489244444444444,
      "loss": 2.6348,
      "step": 15500
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 1.7291572093963623,
      "learning_rate": 0.00024311466666666664,
      "loss": 2.6298,
      "step": 16000
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 1.3129682540893555,
      "learning_rate": 0.00024133688888888888,
      "loss": 2.6316,
      "step": 16500
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 1.5772790908813477,
      "learning_rate": 0.0002395591111111111,
      "loss": 2.6186,
      "step": 17000
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 1.645735263824463,
      "learning_rate": 0.0002377813333333333,
      "loss": 2.6217,
      "step": 17500
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.3946764469146729,
      "learning_rate": 0.00023600355555555554,
      "loss": 2.6178,
      "step": 18000
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 1.6330647468566895,
      "learning_rate": 0.00023422577777777774,
      "loss": 2.6117,
      "step": 18500
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 1.737061858177185,
      "learning_rate": 0.00023244799999999998,
      "loss": 2.5973,
      "step": 19000
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 1.389589786529541,
      "learning_rate": 0.00023067022222222222,
      "loss": 2.5903,
      "step": 19500
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.6553419828414917,
      "learning_rate": 0.0002288924444444444,
      "loss": 2.5823,
      "step": 20000
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 1.4266607761383057,
      "learning_rate": 0.00022711466666666664,
      "loss": 2.6153,
      "step": 20500
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 1.4774127006530762,
      "learning_rate": 0.00022533688888888887,
      "loss": 2.5871,
      "step": 21000
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 1.7755084037780762,
      "learning_rate": 0.00022355911111111108,
      "loss": 2.6077,
      "step": 21500
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 1.6139909029006958,
      "learning_rate": 0.00022178133333333332,
      "loss": 2.606,
      "step": 22000
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.8164957761764526,
      "learning_rate": 0.00022000355555555555,
      "loss": 2.5938,
      "step": 22500
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 1.7263003587722778,
      "learning_rate": 0.00021822577777777774,
      "loss": 2.5747,
      "step": 23000
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 1.6998053789138794,
      "learning_rate": 0.00021644799999999997,
      "loss": 2.5698,
      "step": 23500
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 1.722904920578003,
      "learning_rate": 0.0002146702222222222,
      "loss": 2.5896,
      "step": 24000
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 1.6535638570785522,
      "learning_rate": 0.00021289244444444442,
      "loss": 2.6035,
      "step": 24500
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.607591152191162,
      "learning_rate": 0.00021111466666666665,
      "loss": 2.6117,
      "step": 25000
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 1.507310390472412,
      "learning_rate": 0.0002093368888888889,
      "loss": 2.5604,
      "step": 25500
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 1.5902297496795654,
      "learning_rate": 0.0002075591111111111,
      "loss": 2.5762,
      "step": 26000
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 1.6700539588928223,
      "learning_rate": 0.0002057813333333333,
      "loss": 2.5775,
      "step": 26500
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.4412901401519775,
      "learning_rate": 0.00020400355555555554,
      "loss": 2.5752,
      "step": 27000
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.6531658172607422,
      "learning_rate": 0.00020222577777777778,
      "loss": 2.5533,
      "step": 27500
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 1.5364824533462524,
      "learning_rate": 0.00020044799999999996,
      "loss": 2.5938,
      "step": 28000
    },
    {
      "epoch": 1.0,
      "eval_bleu": 54.53667069636045,
      "eval_loss": 2.3142964839935303,
      "eval_runtime": 11501.3509,
      "eval_samples_per_second": 4.347,
      "eval_steps_per_second": 0.543,
      "step": 28125
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 1.3471931219100952,
      "learning_rate": 0.0001986702222222222,
      "loss": 2.5594,
      "step": 28500
    },
    {
      "epoch": 1.031111111111111,
      "grad_norm": 1.5758333206176758,
      "learning_rate": 0.00019689244444444443,
      "loss": 2.5345,
      "step": 29000
    },
    {
      "epoch": 1.048888888888889,
      "grad_norm": 1.5358331203460693,
      "learning_rate": 0.00019511466666666664,
      "loss": 2.559,
      "step": 29500
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1.3771203756332397,
      "learning_rate": 0.00019333688888888888,
      "loss": 2.5653,
      "step": 30000
    },
    {
      "epoch": 1.0844444444444445,
      "grad_norm": 1.4088059663772583,
      "learning_rate": 0.00019155911111111112,
      "loss": 2.5715,
      "step": 30500
    },
    {
      "epoch": 1.1022222222222222,
      "grad_norm": 1.7732069492340088,
      "learning_rate": 0.0001897813333333333,
      "loss": 2.546,
      "step": 31000
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.70487380027771,
      "learning_rate": 0.00018800355555555553,
      "loss": 2.5637,
      "step": 31500
    },
    {
      "epoch": 1.1377777777777778,
      "grad_norm": 1.611403465270996,
      "learning_rate": 0.00018622577777777777,
      "loss": 2.5683,
      "step": 32000
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 1.2415779829025269,
      "learning_rate": 0.00018444799999999998,
      "loss": 2.5385,
      "step": 32500
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 1.4519286155700684,
      "learning_rate": 0.00018267022222222222,
      "loss": 2.544,
      "step": 33000
    },
    {
      "epoch": 1.1911111111111112,
      "grad_norm": 1.6459851264953613,
      "learning_rate": 0.00018089244444444445,
      "loss": 2.55,
      "step": 33500
    },
    {
      "epoch": 1.208888888888889,
      "grad_norm": 1.7283068895339966,
      "learning_rate": 0.00017911466666666663,
      "loss": 2.545,
      "step": 34000
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 1.6819790601730347,
      "learning_rate": 0.00017733688888888887,
      "loss": 2.5416,
      "step": 34500
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 1.536258339881897,
      "learning_rate": 0.0001755591111111111,
      "loss": 2.5546,
      "step": 35000
    },
    {
      "epoch": 1.2622222222222224,
      "grad_norm": 1.973300576210022,
      "learning_rate": 0.00017378133333333332,
      "loss": 2.5478,
      "step": 35500
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.7995525598526,
      "learning_rate": 0.00017200355555555555,
      "loss": 2.5496,
      "step": 36000
    },
    {
      "epoch": 1.2977777777777777,
      "grad_norm": 1.574891209602356,
      "learning_rate": 0.00017022577777777776,
      "loss": 2.5419,
      "step": 36500
    },
    {
      "epoch": 1.3155555555555556,
      "grad_norm": 1.5850472450256348,
      "learning_rate": 0.00016844799999999997,
      "loss": 2.5442,
      "step": 37000
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.7531511783599854,
      "learning_rate": 0.0001666702222222222,
      "loss": 2.5453,
      "step": 37500
    },
    {
      "epoch": 1.3511111111111112,
      "grad_norm": 1.4629987478256226,
      "learning_rate": 0.00016489244444444444,
      "loss": 2.5417,
      "step": 38000
    },
    {
      "epoch": 1.3688888888888888,
      "grad_norm": 1.8560582399368286,
      "learning_rate": 0.00016311466666666662,
      "loss": 2.5386,
      "step": 38500
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 1.8570165634155273,
      "learning_rate": 0.00016133688888888886,
      "loss": 2.552,
      "step": 39000
    },
    {
      "epoch": 1.4044444444444444,
      "grad_norm": 1.4303302764892578,
      "learning_rate": 0.0001595591111111111,
      "loss": 2.5326,
      "step": 39500
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 1.9541009664535522,
      "learning_rate": 0.0001577813333333333,
      "loss": 2.5308,
      "step": 40000
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.9506759643554688,
      "learning_rate": 0.00015600355555555554,
      "loss": 2.53,
      "step": 40500
    },
    {
      "epoch": 1.4577777777777778,
      "grad_norm": 1.86001718044281,
      "learning_rate": 0.00015422577777777778,
      "loss": 2.5344,
      "step": 41000
    },
    {
      "epoch": 1.4755555555555555,
      "grad_norm": 1.694183349609375,
      "learning_rate": 0.00015244799999999996,
      "loss": 2.5298,
      "step": 41500
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 1.7428890466690063,
      "learning_rate": 0.0001506702222222222,
      "loss": 2.5258,
      "step": 42000
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 1.9321191310882568,
      "learning_rate": 0.00014889244444444443,
      "loss": 2.55,
      "step": 42500
    },
    {
      "epoch": 1.528888888888889,
      "grad_norm": 1.8522961139678955,
      "learning_rate": 0.00014711466666666664,
      "loss": 2.5253,
      "step": 43000
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 1.333526849746704,
      "learning_rate": 0.00014533688888888888,
      "loss": 2.5136,
      "step": 43500
    },
    {
      "epoch": 1.5644444444444443,
      "grad_norm": 1.5187586545944214,
      "learning_rate": 0.00014355911111111111,
      "loss": 2.518,
      "step": 44000
    },
    {
      "epoch": 1.5822222222222222,
      "grad_norm": 1.8120182752609253,
      "learning_rate": 0.00014178133333333332,
      "loss": 2.5088,
      "step": 44500
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.8673043251037598,
      "learning_rate": 0.00014000355555555553,
      "loss": 2.5195,
      "step": 45000
    },
    {
      "epoch": 1.6177777777777778,
      "grad_norm": 1.7740367650985718,
      "learning_rate": 0.00013822577777777777,
      "loss": 2.5122,
      "step": 45500
    },
    {
      "epoch": 1.6355555555555554,
      "grad_norm": 1.7588971853256226,
      "learning_rate": 0.00013644799999999998,
      "loss": 2.5114,
      "step": 46000
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 1.6199365854263306,
      "learning_rate": 0.00013467022222222221,
      "loss": 2.5316,
      "step": 46500
    },
    {
      "epoch": 1.6711111111111112,
      "grad_norm": 2.2808239459991455,
      "learning_rate": 0.00013289244444444442,
      "loss": 2.5134,
      "step": 47000
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 1.4333617687225342,
      "learning_rate": 0.00013111466666666666,
      "loss": 2.4993,
      "step": 47500
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 1.619895577430725,
      "learning_rate": 0.0001293368888888889,
      "loss": 2.5049,
      "step": 48000
    },
    {
      "epoch": 1.7244444444444444,
      "grad_norm": 1.7621161937713623,
      "learning_rate": 0.0001275591111111111,
      "loss": 2.5085,
      "step": 48500
    },
    {
      "epoch": 1.7422222222222223,
      "grad_norm": 1.4764385223388672,
      "learning_rate": 0.00012578133333333331,
      "loss": 2.5329,
      "step": 49000
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.4604912996292114,
      "learning_rate": 0.00012400355555555555,
      "loss": 2.5063,
      "step": 49500
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 1.4671608209609985,
      "learning_rate": 0.00012222577777777776,
      "loss": 2.5363,
      "step": 50000
    },
    {
      "epoch": 1.7955555555555556,
      "grad_norm": 1.7106252908706665,
      "learning_rate": 0.00012044799999999998,
      "loss": 2.5132,
      "step": 50500
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 1.7212698459625244,
      "learning_rate": 0.00011867022222222222,
      "loss": 2.4943,
      "step": 51000
    },
    {
      "epoch": 1.8311111111111111,
      "grad_norm": 1.7250615358352661,
      "learning_rate": 0.00011689244444444443,
      "loss": 2.5073,
      "step": 51500
    },
    {
      "epoch": 1.8488888888888888,
      "grad_norm": 1.5714296102523804,
      "learning_rate": 0.00011511466666666665,
      "loss": 2.5165,
      "step": 52000
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.478371262550354,
      "learning_rate": 0.00011333688888888889,
      "loss": 2.5403,
      "step": 52500
    },
    {
      "epoch": 1.8844444444444446,
      "grad_norm": 1.7691680192947388,
      "learning_rate": 0.0001115591111111111,
      "loss": 2.4927,
      "step": 53000
    },
    {
      "epoch": 1.9022222222222223,
      "grad_norm": 1.4588961601257324,
      "learning_rate": 0.00010978133333333332,
      "loss": 2.4942,
      "step": 53500
    },
    {
      "epoch": 1.92,
      "grad_norm": 2.993007183074951,
      "learning_rate": 0.00010800355555555555,
      "loss": 2.5049,
      "step": 54000
    },
    {
      "epoch": 1.9377777777777778,
      "grad_norm": 1.7102665901184082,
      "learning_rate": 0.00010622577777777776,
      "loss": 2.4957,
      "step": 54500
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 1.5708239078521729,
      "learning_rate": 0.000104448,
      "loss": 2.5316,
      "step": 55000
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 1.776769757270813,
      "learning_rate": 0.00010267022222222221,
      "loss": 2.5132,
      "step": 55500
    },
    {
      "epoch": 1.991111111111111,
      "grad_norm": 1.6723642349243164,
      "learning_rate": 0.00010089244444444443,
      "loss": 2.5293,
      "step": 56000
    },
    {
      "epoch": 2.0,
      "eval_bleu": 60.55732074232137,
      "eval_loss": 2.2573838233947754,
      "eval_runtime": 11457.6497,
      "eval_samples_per_second": 4.364,
      "eval_steps_per_second": 0.545,
      "step": 56250
    }
  ],
  "logging_steps": 500,
  "max_steps": 84375,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.74037249032192e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
