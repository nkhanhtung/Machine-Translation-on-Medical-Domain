{
  "best_global_step": 28125,
  "best_metric": 2.7203822135925293,
  "best_model_checkpoint": "opus-mt-vi-en-medical-finetuned/checkpoint-28125",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 28125,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 1.1085819005966187,
      "learning_rate": 0.0002946773333333333,
      "loss": 3.6198,
      "step": 500
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 1.2951163053512573,
      "learning_rate": 0.00028934399999999997,
      "loss": 3.443,
      "step": 1000
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 1.2529926300048828,
      "learning_rate": 0.00028401066666666665,
      "loss": 3.3872,
      "step": 1500
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 1.4290121793746948,
      "learning_rate": 0.0002786773333333333,
      "loss": 3.346,
      "step": 2000
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 1.153687596321106,
      "learning_rate": 0.00027334399999999996,
      "loss": 3.2946,
      "step": 2500
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 1.5403023958206177,
      "learning_rate": 0.00026801066666666664,
      "loss": 3.2807,
      "step": 3000
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 1.349472999572754,
      "learning_rate": 0.0002626773333333333,
      "loss": 3.2407,
      "step": 3500
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 1.8394310474395752,
      "learning_rate": 0.00025734399999999995,
      "loss": 3.2416,
      "step": 4000
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.6554313898086548,
      "learning_rate": 0.00025201066666666664,
      "loss": 3.1719,
      "step": 4500
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.3167458772659302,
      "learning_rate": 0.0002466773333333333,
      "loss": 3.1987,
      "step": 5000
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 1.4578897953033447,
      "learning_rate": 0.00024134399999999997,
      "loss": 3.166,
      "step": 5500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 1.405064582824707,
      "learning_rate": 0.00023601066666666663,
      "loss": 3.1812,
      "step": 6000
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 1.510789394378662,
      "learning_rate": 0.0002306773333333333,
      "loss": 3.1515,
      "step": 6500
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 1.458217740058899,
      "learning_rate": 0.00022534399999999996,
      "loss": 3.1552,
      "step": 7000
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.6007981300354004,
      "learning_rate": 0.00022001066666666664,
      "loss": 3.1228,
      "step": 7500
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 1.3978049755096436,
      "learning_rate": 0.0002146773333333333,
      "loss": 3.1021,
      "step": 8000
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 1.3683816194534302,
      "learning_rate": 0.00020934399999999998,
      "loss": 3.1126,
      "step": 8500
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.517465591430664,
      "learning_rate": 0.00020401066666666663,
      "loss": 3.1312,
      "step": 9000
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 1.7428202629089355,
      "learning_rate": 0.0001986773333333333,
      "loss": 3.0976,
      "step": 9500
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 1.4431049823760986,
      "learning_rate": 0.000193344,
      "loss": 3.0725,
      "step": 10000
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.5366649627685547,
      "learning_rate": 0.00018801066666666665,
      "loss": 3.1055,
      "step": 10500
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 1.7669706344604492,
      "learning_rate": 0.00018267733333333333,
      "loss": 3.0889,
      "step": 11000
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 1.6375373601913452,
      "learning_rate": 0.000177344,
      "loss": 3.0932,
      "step": 11500
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 1.3846631050109863,
      "learning_rate": 0.00017201066666666667,
      "loss": 3.0674,
      "step": 12000
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.5848687887191772,
      "learning_rate": 0.00016667733333333332,
      "loss": 3.067,
      "step": 12500
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 1.6413934230804443,
      "learning_rate": 0.000161344,
      "loss": 3.0389,
      "step": 13000
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.5847883224487305,
      "learning_rate": 0.00015601066666666666,
      "loss": 3.0572,
      "step": 13500
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 1.3568142652511597,
      "learning_rate": 0.00015067733333333331,
      "loss": 3.0698,
      "step": 14000
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 1.6874099969863892,
      "learning_rate": 0.000145344,
      "loss": 3.0529,
      "step": 14500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.4322662353515625,
      "learning_rate": 0.00014001066666666665,
      "loss": 3.0424,
      "step": 15000
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 1.6093776226043701,
      "learning_rate": 0.00013467733333333333,
      "loss": 3.0378,
      "step": 15500
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 1.4985560178756714,
      "learning_rate": 0.000129344,
      "loss": 3.0519,
      "step": 16000
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 1.4932247400283813,
      "learning_rate": 0.00012401066666666667,
      "loss": 3.0426,
      "step": 16500
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 1.5552022457122803,
      "learning_rate": 0.00011867733333333332,
      "loss": 3.0523,
      "step": 17000
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 1.6197623014450073,
      "learning_rate": 0.00011334399999999999,
      "loss": 3.027,
      "step": 17500
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.4011552333831787,
      "learning_rate": 0.00010801066666666666,
      "loss": 3.0434,
      "step": 18000
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 1.6001275777816772,
      "learning_rate": 0.00010267733333333333,
      "loss": 3.0265,
      "step": 18500
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 1.641628384590149,
      "learning_rate": 9.7344e-05,
      "loss": 3.0256,
      "step": 19000
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 1.3489019870758057,
      "learning_rate": 9.201066666666665e-05,
      "loss": 3.0216,
      "step": 19500
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.6517244577407837,
      "learning_rate": 8.667733333333332e-05,
      "loss": 3.0027,
      "step": 20000
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 1.684129238128662,
      "learning_rate": 8.134399999999999e-05,
      "loss": 3.0188,
      "step": 20500
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 1.5637702941894531,
      "learning_rate": 7.601066666666665e-05,
      "loss": 3.0051,
      "step": 21000
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 1.898241400718689,
      "learning_rate": 7.067733333333332e-05,
      "loss": 3.0151,
      "step": 21500
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 1.6970523595809937,
      "learning_rate": 6.534399999999999e-05,
      "loss": 3.033,
      "step": 22000
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.6996119022369385,
      "learning_rate": 6.001066666666666e-05,
      "loss": 3.0225,
      "step": 22500
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 2.4185588359832764,
      "learning_rate": 5.4677333333333326e-05,
      "loss": 2.9942,
      "step": 23000
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 1.7370069026947021,
      "learning_rate": 4.9343999999999994e-05,
      "loss": 2.9888,
      "step": 23500
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 1.520344853401184,
      "learning_rate": 4.401066666666667e-05,
      "loss": 3.0207,
      "step": 24000
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 1.6313393115997314,
      "learning_rate": 3.867733333333333e-05,
      "loss": 3.02,
      "step": 24500
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.7244926691055298,
      "learning_rate": 3.3344e-05,
      "loss": 3.0196,
      "step": 25000
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 2.0263662338256836,
      "learning_rate": 2.8010666666666666e-05,
      "loss": 2.9906,
      "step": 25500
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 1.6058107614517212,
      "learning_rate": 2.267733333333333e-05,
      "loss": 3.0062,
      "step": 26000
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 1.7020795345306396,
      "learning_rate": 1.7344e-05,
      "loss": 3.013,
      "step": 26500
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.7963645458221436,
      "learning_rate": 1.2010666666666665e-05,
      "loss": 2.9798,
      "step": 27000
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.9581879377365112,
      "learning_rate": 6.677333333333333e-06,
      "loss": 2.9905,
      "step": 27500
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 1.3343803882598877,
      "learning_rate": 1.3439999999999998e-06,
      "loss": 3.0265,
      "step": 28000
    },
    {
      "epoch": 1.0,
      "eval_bleu": 51.92447498837859,
      "eval_loss": 2.7203822135925293,
      "eval_runtime": 12246.4372,
      "eval_samples_per_second": 4.083,
      "eval_steps_per_second": 0.51,
      "step": 28125
    }
  ],
  "logging_steps": 500,
  "max_steps": 28125,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9029409012449280.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
